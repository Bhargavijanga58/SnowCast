[{
  "history_id" : "101g9m95lew",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696428,
  "history_end_time" : 1667410696428,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a7lmrekn3w9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696428,
  "history_end_time" : 1667410696428,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3wcis04wqbh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696429,
  "history_end_time" : 1667410696429,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wtx2g3pf0jc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696429,
  "history_end_time" : 1667410696429,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7bfahhxnrk2",
  "history_input" : "# Find the best model\nprint(\"model comparison script\")\nprint(\"hello world\")",
  "history_output" : "model comparison script\nhello world\n",
  "history_begin_time" : 1667410700438,
  "history_end_time" : 1667410700524,
  "history_notes" : null,
  "history_process" : "mi3e5n",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "2xw63xiztw5",
  "history_input" : "# Integrate all the datasets into one training dataset\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\nfrom datetime import datetime\n\n\nprint(\"integrating datasets into one dataset\")\n#pd.set_option('display.max_columns', None)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\n#example_mod_file = f\"{github_dir}/data/modis/mod10a1_ndsi_f191fe19-0e81-4bc9-9980-29738a05a49b.csv\"\n\n\ntraining_feature_pd = pd.read_csv(training_feature_file, header=0, index_col=0)\ntesting_feature_pd = pd.read_csv(testing_feature_file, header=0, index_col=0)\ntrain_labels_pd = pd.read_csv(train_labels_file, header=0, index_col=0)\n#print(training_feature_pd.head())\n\nstation_cell_mapper_pd = pd.read_csv(station_cell_mapper_file, header=0, index_col=0)\n#print(station_cell_mapper_pd.head())\n\n#example_mod_pd = pd.read_csv(example_mod_file, header=0, index_col=0)\n#print(example_mod_pd.shape)\ndef getDateStr(x):\n  return x.split(\" \")[0]\n\ndef integrate_modis():\n  \"\"\"\n  Integrate all MODIS data into mod_all.csv\n  \"\"\"\n  all_mod_file = f\"{github_dir}/data/ready_for_training/modis_all.csv\"\n  if os.path.isfile(all_mod_file):\n    return\n  dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n  mod_all_df = pd.DataFrame(columns=[\"date\"])\n  mod_all_df['date'] = dates\n  \n  #print(mod_all_df.head())\n  for ind in station_cell_mapper_pd.index:\n    current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n    print(current_cell_id)\n    mod_single_file = f\"{github_dir}/data/sat_training/modis/mod10a1_ndsi_{current_cell_id}.csv\"\n    if os.path.isfile(mod_single_file):\n      mod_single_pd = pd.read_csv(mod_single_file, header=0)\n      mod_single_pd = mod_single_pd[[\"date\", \"mod10a1_ndsi\"]]\n      mod_single_pd = mod_single_pd.rename(columns={\"mod10a1_ndsi\": current_cell_id})\n      mod_single_pd['date'] = pd.to_datetime(mod_single_pd['date']).astype(str)\n      print(mod_all_df.shape)\n      mod_all_df = pd.merge(mod_all_df, mod_single_pd, how='left', on=\"date\")\n  mod_all_df.to_csv(all_mod_file)\n\n  \ndef integrate_sentinel1():\n  \"\"\"\n  Integrate all Sentinel 1 data into sentinel1_all.csv\n  \"\"\"\n  all_sentinel1_file = f\"{github_dir}/data/ready_for_training/sentinel1_all.csv\"\n  if os.path.isfile(all_sentinel1_file):\n    return\n  dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n  sentinel1_all_df = pd.DataFrame(columns=[\"date\"])\n  sentinel1_all_df['date'] = dates\n  #print(mod_all_df.head())\n  \n  for ind in station_cell_mapper_pd.index:\n    current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n    print(current_cell_id)\n    sentinel1_single_file = f\"{github_dir}/data/sat_training/sentinel1/s1_grd_vv_{current_cell_id}.csv\"\n    if os.path.isfile(sentinel1_single_file) and current_cell_id not in sentinel1_all_df :\n      sentinel1_single_pd = pd.read_csv(sentinel1_single_file, header=0)\n      sentinel1_single_pd = sentinel1_single_pd[[\"date\", \"s1_grd_vv\"]]\n      sentinel1_single_pd = sentinel1_single_pd.rename(columns={\"s1_grd_vv\": current_cell_id})\n      #sentinel1_single_pd['date'] = sentinel1_single_pd['date'].astype('datetime64[ns]')\n      sentinel1_single_pd['date'] = pd.to_datetime(sentinel1_single_pd['date']).dt.round(\"D\").astype(str)\n      print(\"sentinel1_single_pd: \", sentinel1_single_pd.head())\n      print(\"sentinel1_single_pd check value: \", sentinel1_single_pd[sentinel1_single_pd[\"date\"]==\"2015-04-01\"])\n      sentinel1_single_pd = sentinel1_single_pd.drop_duplicates(subset=['date'], keep='first') # this will remove all the other values of the same day\n      \n      sentinel1_all_df = pd.merge(sentinel1_all_df, sentinel1_single_pd, how='left', on=\"date\")\n      print(\"sentinel1_all_df check value: \", sentinel1_all_df[sentinel1_all_df[\"date\"]==\"2015-04-01\"])\n      print(\"sentinel1_all_df: \", sentinel1_all_df.shape)\n      \n\n  print(sentinel1_all_df.shape)\n  sentinel1_all_df.to_csv(all_sentinel1_file)\n\ndef integrate_gridmet():\n  \"\"\"\n  Integrate all gridMET data into gridmet_all.csv\n  \"\"\"\n  \n  \n  dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n  \n  #print(mod_all_df.head())\n  var_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n  \n  for var in var_list:\n    gridmet_all_df = pd.DataFrame(columns=[\"date\"])\n    gridmet_all_df['date'] = dates\n    all_gridmet_file = f\"{github_dir}/data/ready_for_training/gridmet_{var}_all.csv\"\n    if os.path.isfile(all_gridmet_file):\n      return\n    for ind in station_cell_mapper_pd.index:\n      current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n      print(current_cell_id)\n      gridmet_single_file = f\"{github_dir}/data/sim_training/gridmet/{var}_{current_cell_id}.csv\"\n      if os.path.isfile(gridmet_single_file) and current_cell_id not in gridmet_all_df :\n        gridmet_single_pd = pd.read_csv(gridmet_single_file, header=0)\n        gridmet_single_pd = gridmet_single_pd[[\"date\", var]]\n        gridmet_single_pd = gridmet_single_pd.rename(columns={var: current_cell_id})\n        #sentinel1_single_pd['date'] = sentinel1_single_pd['date'].astype('datetime64[ns]')\n        gridmet_single_pd['date'] = pd.to_datetime(gridmet_single_pd['date']).dt.round(\"D\").astype(str)\n        print(\"gridmet_single_pd: \", gridmet_single_pd.head())\n        print(\"gridmet_single_pd check value: \", gridmet_single_pd[gridmet_single_pd[\"date\"]==\"2015-04-01\"])\n        gridmet_single_pd = gridmet_single_pd.drop_duplicates(subset=['date'], keep='first') # this will remove all the other values of the same day\n\n        gridmet_all_df = pd.merge(gridmet_all_df, gridmet_single_pd, how='left', on=\"date\")\n        print(\"gridmet_all_df check value: \", gridmet_all_df[gridmet_all_df[\"date\"]==\"2015-04-01\"])\n        print(\"gridmet_all_df: \", gridmet_all_df.shape)\n      \n    print(gridmet_all_df.shape)\n    gridmet_all_df.to_csv(all_gridmet_file)\n  \n  \ndef prepare_training_csv():\n  \"\"\"\n  MOD model:\n    input columns: [m, doy, ndsi]\n    output column: [swe]\n  Sentinel1 model:\n    input columns: [m, doy, grd]\n    output column: [swe]\n  gridMET model:\n    input columns: [m, doy, tmmn, tmmx, pr, vpd, eto, rmax, rmin, vs]\n    output column: [swe]\n  \"\"\"\n  all_ready_file = f\"{github_dir}/data/ready_for_training/all_ready.csv\"\n  if os.path.isfile(all_ready_file):\n      return\n  \n  all_mod_file = f\"{github_dir}/data/ready_for_training/modis_all.csv\"\n  modis_all_pd = pd.read_csv(all_mod_file, header=0)\n  all_sentinel1_file = f\"{github_dir}/data/ready_for_training/sentinel1_all.csv\"\n  sentinel1_all_pd = pd.read_csv(all_sentinel1_file, header=0)\n  all_gridmet_eto_file = f\"{github_dir}/data/ready_for_training/gridmet_eto_all.csv\"\n  gridmet_eto_all_pd = pd.read_csv(all_gridmet_eto_file, header=0, index_col = 0)\n  all_gridmet_pr_file = f\"{github_dir}/data/ready_for_training/gridmet_pr_all.csv\"\n  gridmet_pr_all_pd = pd.read_csv(all_gridmet_pr_file, header=0, index_col = 0)\n  all_gridmet_rmax_file = f\"{github_dir}/data/ready_for_training/gridmet_rmax_all.csv\"\n  gridmet_rmax_all_pd = pd.read_csv(all_gridmet_rmax_file, header=0, index_col = 0)\n  all_gridmet_rmin_file = f\"{github_dir}/data/ready_for_training/gridmet_rmin_all.csv\"\n  gridmet_rmin_all_pd = pd.read_csv(all_gridmet_rmin_file, header=0, index_col = 0)\n  all_gridmet_tmmn_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmn_all.csv\"\n  gridmet_tmmn_all_pd = pd.read_csv(all_gridmet_tmmn_file, header=0, index_col = 0)\n  all_gridmet_tmmx_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmx_all.csv\"\n  gridmet_tmmx_all_pd = pd.read_csv(all_gridmet_tmmx_file, header=0, index_col = 0)\n  all_gridmet_vpd_file = f\"{github_dir}/data/ready_for_training/gridmet_vpd_all.csv\"\n  gridmet_vpd_all_pd = pd.read_csv(all_gridmet_vpd_file, header=0, index_col = 0)\n  all_gridmet_vs_file = f\"{github_dir}/data/ready_for_training/gridmet_vs_all.csv\"\n  gridmet_vs_all_pd = pd.read_csv(all_gridmet_vs_file, header=0, index_col = 0)\n  \n  grid_terrain_file = f\"{github_dir}/data/terrain/gridcells_terrainData.csv\"\n  grid_terrain_pd = pd.read_csv(grid_terrain_file, header=0, index_col = 1)\n  \n  print(\"modis_all_size: \", modis_all_pd.shape)\n  print(\"station size: \", station_cell_mapper_pd.shape)\n  print(\"training_feature_pd size: \", training_feature_pd.shape)\n  print(\"testing_feature_pd size: \", testing_feature_pd.shape)\n  \n  all_training_pd = pd.DataFrame(columns=[\"cell_id\", \"year\", \"m\", \"doy\", \"ndsi\", \"grd\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"lat\", \"lon\", \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\", \"swe\"])\n  all_training_pd = all_training_pd.reset_index()\n  for index, row in modis_all_pd.iterrows():\n    dt = datetime.strptime(row['date'], '%Y-%m-%d')\n    month = dt.month\n    year = dt.year\n    doy = dt.timetuple().tm_yday\n    print(f\"Dealing {year} {doy}\")\n    for i in range(3,len(row.index)):\n      cell_id = row.index[i][:-2]\n      if cell_id in train_labels_pd.index and row['date'] in train_labels_pd:\n        ndsi = row.values[i]\n        swe = train_labels_pd.loc[cell_id, row['date']]\n        grd = sentinel1_all_pd.loc[index, cell_id]\n        eto = gridmet_eto_all_pd.loc[index, cell_id]\n        pr = gridmet_pr_all_pd.loc[index, cell_id]\n        rmax = gridmet_rmax_all_pd.loc[index, cell_id]\n        rmin = gridmet_rmin_all_pd.loc[index, cell_id]\n        tmmn = gridmet_tmmn_all_pd.loc[index, cell_id]\n        tmmx = gridmet_tmmx_all_pd.loc[index, cell_id]\n        vpd = gridmet_vpd_all_pd.loc[index, cell_id]\n        vs = gridmet_vs_all_pd.loc[index, cell_id]\n        lat = grid_terrain_pd.loc[cell_id, \"Longitude [deg]\"]\n        lon = grid_terrain_pd.loc[cell_id, \"Latitude [deg]\"]\n        elevation = grid_terrain_pd.loc[cell_id, \"Elevation [m]\"]\n        aspect = grid_terrain_pd.loc[cell_id, \"Aspect [deg]\"]\n        curvature = grid_terrain_pd.loc[cell_id, \"Curvature [ratio]\"]\n        slope = grid_terrain_pd.loc[cell_id, \"Slope [deg]\"]\n        eastness = grid_terrain_pd.loc[cell_id, \"Eastness [unitCirc.]\"]\n        northness = grid_terrain_pd.loc[cell_id, \"Northness [unitCirc.]\"]\n        \n        if not np.isnan(swe):\n          json_kv = {\"cell_id\": cell_id, \"year\":year, \"m\":month, \"doy\": doy, \"ndsi\":ndsi, \"grd\":grd, \"eto\":eto, \"pr\":pr, \"rmax\":rmax, \"rmin\":rmin, \"tmmn\":tmmn, \"tmmx\":tmmx, \"vpd\":vpd, \"vs\":vs, \"lat\":lat, \"lon\":lon, \"elevation\":elevation, \"aspect\":aspect, \"curvature\":curvature, \"slope\":slope, \"eastness\":eastness, \"northness\":northness, \"swe\":swe}\n          # print(json_kv)\n          all_training_pd = all_training_pd.append(json_kv, ignore_index = True)\n  \n  print(all_training_pd.shape)\n  all_training_pd.to_csv(all_ready_file)\n  \n  \"\"\"\n  grd_all_pd = pd.DataFrame(columns=[\"year\", \"m\", \"doy\", \"grd\", \"swe\"])\n  grd_all_pd = grd_all_pd.reset_index()\n  for index, row in sentinel1_all_pd.iterrows():\n    dt = datetime.strptime(row['date'], '%Y-%m-%d')\n    year = dt.year\n    month = dt.month\n    doy = dt.timetuple().tm_yday\n    for i in range(3,len(row.index)):\n      cell_id = row.index[i]\n      grd = row.values[i]\n      if not np.isnan(grd) and cell_id in train_labels_pd.index and row['date'] in train_labels_pd:\n        swe = train_labels_pd.loc[cell_id, row['date']]\n        if not np.isnan(swe):\n          print([month, doy, grd, swe])\n          grd_all_pd = grd_all_pd.append({\"year\": year, \"m\":month, \"doy\": doy, \"grd\": grd, \"swe\": swe}, ignore_index = True)\n  \n  print(grd_all_pd.shape)\n  grd_all_pd.to_csv(f\"{github_dir}/data/ready_for_training/sentinel1_ready.csv\")\n  \"\"\"\n  \n#exit() # done already\n\n#integrate_modis()\n#integrate_sentinel1()\n#integrate_gridmet()\n#prepare_training_csv()\n\n\n  \n  \n  \n",
  "history_output" : "C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py:148: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n  from . import _distributor_init\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\2xw63xiztw5\\data_integration.py\", line 3, in <module>\n    import pandas as pd\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\pandas\\__init__.py\", line 16, in <module>\n    raise ImportError(\nImportError: Unable to import required dependencies:\nnumpy: \nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\nWe have compiled some common reasons and troubleshooting tips at:\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\nPlease note and check the following:\n  * The Python version is: Python3.9 from \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\python.exe\"\n  * The NumPy version is: \"1.21.5\"\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
  "history_begin_time" : 1667410699024,
  "history_end_time" : 1667410699141,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5bxsbtxz478",
  "history_input" : "# Deploy model to service\n\nprint(\"deploy model to service\")\n",
  "history_output" : "deploy model to service\n",
  "history_begin_time" : 1667410701289,
  "history_end_time" : 1667410701377,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "gwazkr1749p",
  "history_input" : "# Predict results using the model\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nexit()  # for now, the workflow is not ready yet\n\n# read the grid geometry file\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic.joblib\")\nbasic_predicted_values = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest.joblib\")\nrandom_predicted_values = evaluate(best_random, all_features, all_labels, \"Optimized\")\n",
  "history_output" : "C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py:148: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n  from . import _distributor_init\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\__init__.py\", line 22, in <module>\n    from . import multiarray\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\multiarray.py\", line 12, in <module>\n    from . import overrides\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\overrides.py\", line 7, in <module>\n    from numpy.core._multiarray_umath import (\nImportError: DLL load failed while importing _multiarray_umath: The specified module could not be found.\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\gwazkr1749p\\service_prediction.py\", line 3, in <module>\n    from sklearn.ensemble import RandomForestRegressor\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\sklearn\\__init__.py\", line 82, in <module>\n    from .base import clone\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\sklearn\\base.py\", line 13, in <module>\n    import numpy as np\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py\", line 150, in <module>\n    from . import core\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\__init__.py\", line 48, in <module>\n    raise ImportError(msg)\nImportError: \nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\nWe have compiled some common reasons and troubleshooting tips at:\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\nPlease note and check the following:\n  * The Python version is: Python3.9 from \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\python.exe\"\n  * The NumPy version is: \"1.21.5\"\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
  "history_begin_time" : 1667410701544,
  "history_end_time" : 1667410701669,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "4cl0i3c0jc8",
  "history_input" : "from model_creation_rf import RandomForestHole\nfrom model_creation_xgboost import XGBoostHole\n\nprint(\"Train Models\")\n\nworm_holes = [RandomForestHole(), XGBoostHole()]\n\nfor hole in worm_holes:\n  hole.preprocessing()\n  print(hole.train_x.dtype)\n  print(hole.train_y.dtype)\n  print(hole.train_x.shape)\n  print(hole.train_y.shape)\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  \nprint(\"Finished training and validating all the models.\")\n",
  "history_output" : "C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py:148: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n  from . import _distributor_init\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\__init__.py\", line 22, in <module>\n    from . import multiarray\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\multiarray.py\", line 12, in <module>\n    from . import overrides\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\overrides.py\", line 7, in <module>\n    from numpy.core._multiarray_umath import (\nImportError: DLL load failed while importing _multiarray_umath: The specified module could not be found.\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\4cl0i3c0jc8\\model_train_validate.py\", line 1, in <module>\n    from model_creation_rf import RandomForestHole\n  File \"C:\\Users\\BLi\\gw-workspace\\4cl0i3c0jc8\\model_creation_rf.py\", line 1, in <module>\n    from sklearn.ensemble import RandomForestRegressor\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\sklearn\\__init__.py\", line 82, in <module>\n    from .base import clone\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\sklearn\\base.py\", line 13, in <module>\n    import numpy as np\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py\", line 150, in <module>\n    from . import core\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\__init__.py\", line 48, in <module>\n    raise ImportError(msg)\nImportError: \nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\nWe have compiled some common reasons and troubleshooting tips at:\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\nPlease note and check the following:\n  * The Python version is: Python3.9 from \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\python.exe\"\n  * The NumPy version is: \"1.21.5\"\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
  "history_begin_time" : 1667410699287,
  "history_end_time" : 1667410699419,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "4mi74pbepg9",
  "history_input" : "# Test models\n\n# Random Forest model creation and save to file\n\nfrom sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom datetime import datetime\n\ndef turn_doy_to_date(year, doy):\n  doy = int(doy)\n  doy = str(doy)\n  doy.rjust(3 + len(doy), '0')\n  #res = datetime.strptime(str(year) + \"-\" + doy, \"%Y-%j\").strftime(\"%m/%d/%Y\")\n  res = datetime.strptime(str(year) + \"-\" + doy, \"%Y-%j\").strftime(\"%Y-%m-%d\")\n  return res\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\ntest_ready_file = f\"{github_dir}/data/ready_for_testing/all_ready_3.csv\"\ntest_ready_pd = pd.read_csv(test_ready_file, header=0, index_col=0)\nsubmission_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_pd = pd.read_csv(submission_file, header=0, index_col=0)\npredicted_file = f\"{homedir}/Documents/GitHub/SnowCast/data/results/wormhole_output_4.csv\"\n\ntrain_cols = ['year','m','doy','ndsi','grd','eto','pr','rmax','rmin','tmmn','tmmx','vpd','vs','lat','lon','elevation','aspect','curvature','slope','eastness','northness']\n\nprint(test_ready_pd.shape)\npd_to_clean = test_ready_pd[train_cols]\nprint(\"PD shape: \", pd_to_clean.shape)\n\ndoy_list = test_ready_pd[\"doy\"].unique()\nprint(doy_list)\n\ndate_list = [turn_doy_to_date(2022, doy_list[i]) for i in range(len(doy_list)) ]\nprint(date_list)\n\nall_features = pd_to_clean.to_numpy()\nall_features = np.nan_to_num(all_features)\nprint(\"train feature shape: \", all_features.shape)\n#all_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\n#all_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\n#base_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic_v2.joblib\")\n#base_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic_v2.joblib\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/SnowCast/model/wormhole_20221305163806.joblib\")\ny_predicted = best_random.predict(all_features)\nprint(y_predicted) #first got daily prediction\n\ntarget_dates = [\"2022-01-13\",\"2022-01-20\",\"2022-01-27\",\"2022-02-03\",\"2022-02-10\",\"2022-02-17\",\"2022-02-24\",\"2022-03-03\",\"2022-03-10\",\"2022-03-17\",\"2022-03-24\",\"2022-03-31\",\"2022-04-07\",\"2022-04-14\",\"2022-04-21\",\"2022-04-28\",\"2022-05-05\",\"2022-05-12\",\"2022-05-19\",\"2022-05-26\",\"2022-06-02\",\"2022-06-09\",\"2022-06-16\",\"2022-06-23\",\"2022-06-30\"]\nprint(\"taregt date list: \", len(target_dates))\n\ndaily_predictions = pd.DataFrame(columns = target_dates, index = submission_pd.index)\nfor i in range(len(y_predicted)):\n  doy = all_features[i][2]\n  #print(doy)\n  ndate = turn_doy_to_date(2022, doy)\n  if ndate in target_dates:\n    cell_id = test_ready_pd[\"cell_id\"].iloc[i]\n    daily_predictions.at[cell_id, ndate] = y_predicted[i]\n  #print(ndate, cell_id)\n  #print(y_predicted[i])\n  \nprint(daily_predictions.shape)\n#daily_predictions = daily_predictions[[\"2022-01-13\"]]\n\nif os.path.exists(predicted_file):\n  os.remove(predicted_file)\n  \ndaily_predictions.fillna(0.0, inplace=True)\ndaily_predictions.to_csv(predicted_file, date_format=\"%Y-%d-%m\")\n\n\n# turn daily into weekly using mean values\n\n\n\n\n\n",
  "history_output" : "C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py:148: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n  from . import _distributor_init\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\__init__.py\", line 22, in <module>\n    from . import multiarray\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\multiarray.py\", line 12, in <module>\n    from . import overrides\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\overrides.py\", line 7, in <module>\n    from numpy.core._multiarray_umath import (\nImportError: DLL load failed while importing _multiarray_umath: The specified module could not be found.\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\4mi74pbepg9\\model_test.py\", line 5, in <module>\n    from sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\sklearn\\__init__.py\", line 82, in <module>\n    from .base import clone\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\sklearn\\base.py\", line 13, in <module>\n    import numpy as np\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py\", line 150, in <module>\n    from . import core\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\core\\__init__.py\", line 48, in <module>\n    raise ImportError(msg)\nImportError: \nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\nWe have compiled some common reasons and troubleshooting tips at:\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\nPlease note and check the following:\n  * The Python version is: Python3.9 from \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\python.exe\"\n  * The NumPy version is: \"1.21.5\"\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
  "history_begin_time" : 1667410700151,
  "history_end_time" : 1667410700277,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "iextf3o9soi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696435,
  "history_end_time" : 1667410696435,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lfers4qfwlk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696436,
  "history_end_time" : 1667410696436,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mebw2tf0vm8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696436,
  "history_end_time" : 1667410696436,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h9ggocgojz8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696436,
  "history_end_time" : 1667410696436,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "09hz1wvxkcf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696437,
  "history_end_time" : 1667410696437,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g4v36t3c8by",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696437,
  "history_end_time" : 1667410696437,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ytyp7o1f8ty",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696437,
  "history_end_time" : 1667410696437,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h4hhnpnuo9l",
  "history_input" : "\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\nexit() # uncomment to download new files\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'gridmet'\nproduct_name = 'IDAHO_EPSCOR/GRIDMET'\nstart_date = '2013-01-01'\nend_date = '2021-12-31'\n\nvar_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n\nfor var in var_list:\n\n    var_name = var\n    column_name = var\n\n    dfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\n    if not os.path.exists(dfolder):\n        os.makedirs(dfolder)\n\n    all_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\n    for ind in station_cell_mapper_df.index:\n\n        try:\n\n          current_cell_id = station_cell_mapper_df['cell_id'][ind]\n          print(\"collecting \", current_cell_id)\n          single_csv_file = f\"{dfolder}/{column_name}_{current_cell_id}.csv\"\n\n          if os.path.exists(single_csv_file):\n              print(\"exists skipping..\")\n              continue\n\n          longitude = station_cell_mapper_df['lon'][ind]\n          latitude = station_cell_mapper_df['lat'][ind]\n\n          # identify a 500 meter buffer around our Point Of Interest (POI)\n          poi = ee.Geometry.Point(longitude, latitude).buffer(1000)\n          viirs = ee.ImageCollection(product_name).filterDate(start_date, end_date).filterBounds(poi).select(var_name)\n\n          def poi_mean(img):\n              reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=1000)\n              mean = reducer.get(var_name)\n              return img.set('date', img.date().format()).set(column_name,mean)\n\n\n          poi_reduced_imgs = viirs.map(poi_mean)\n\n          nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n          # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n          df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n          df['date'] = pd.to_datetime(df['date'])\n          df = df.set_index('date')\n\n          df['cell_id'] = current_cell_id\n          df['latitude'] = latitude\n          df['longitude'] = longitude\n          df.to_csv(single_csv_file)\n\n          df_list = [all_cell_df, df]\n          all_cell_df = pd.concat(df_list) # merge into big dataframe\n\n        except Exception as e:\n\n          print(e)\n          pass\n    \n    all_cell_df.to_csv(f\"{dfolder}/{column_name}.csv\")  \n\n\n",
  "history_output" : "C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py:148: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n  from . import _distributor_init\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\h4hhnpnuo9l\\data_gee_gridmet_station_only.py\", line 3, in <module>\n    import pandas as pd\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\pandas\\__init__.py\", line 16, in <module>\n    raise ImportError(\nImportError: Unable to import required dependencies:\nnumpy: \nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\nWe have compiled some common reasons and troubleshooting tips at:\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\nPlease note and check the following:\n  * The Python version is: Python3.9 from \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\python.exe\"\n  * The NumPy version is: \"1.21.5\"\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
  "history_begin_time" : 1667410696787,
  "history_end_time" : 1667410696903,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "jadyvv48gb1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696439,
  "history_end_time" : 1667410696439,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fniv7iu1snz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696439,
  "history_end_time" : 1667410696439,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zcl43kjvatv",
  "history_input" : "# Integrate all the datasets into one training dataset\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\nfrom datetime import datetime as dt\n\nfrom datetime import date\nfrom snowcast_utils import *\n\npd.set_option('display.max_columns', None)\n\ntoday = date.today()\n\n# dd/mm/YY\nstart_date = \"2022-01-01\"\n#end_date = today.strftime(\"%Y-%m-%d\")\nend_date = findLastStopDate(f\"{github_dir}/data/sim_testing/gridmet/\", \"%Y-%m-%d %H:%M:%S\")\nprint(\"d1 =\", end_date)\n\nprint(\"integrating datasets into one dataset\")\n#pd.set_option('display.max_columns', None)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n\n#example_mod_file = f\"{github_dir}/data/modis/mod10a1_ndsi_f191fe19-0e81-4bc9-9980-29738a05a49b.csv\"\n\n\ntraining_feature_pd = pd.read_csv(training_feature_file, header=0, index_col=0)\ntesting_feature_pd = pd.read_csv(testing_feature_file, header=0, index_col=0)\ntrain_labels_pd = pd.read_csv(train_labels_file, header=0, index_col=0)\nsubmission_format_pd = pd.read_csv(submission_format_file, header=0, index_col=0)\n#print(training_feature_pd.head())\n\nstation_cell_mapper_pd = pd.read_csv(station_cell_mapper_file, header=0, index_col=0)\n#print(station_cell_mapper_pd.head())\n\n#example_mod_pd = pd.read_csv(example_mod_file, header=0, index_col=0)\n#print(example_mod_pd.shape)\ndef getDateStr(x):\n  return x.split(\" \")[0]\n\ndef integrate_modis():\n  \"\"\"\n  Integrate all MODIS data into mod_all.csv. Traverse all the csv files in the sat_testing/modis folder\n  and aggregate them into one file with good headers.\n  \"\"\"\n  all_mod_file = f\"{github_dir}/data/ready_for_testing/modis_all.csv\"\n  ready_mod_file = f\"{github_dir}/data/sat_testing/modis/mod10a1_ndsi_{start_date}_{end_date}.csv\"\n  mod_testing_folder = f\"{github_dir}/data/sat_testing/modis/\"\n  if os.path.exists(all_mod_file):\n    os.remove(all_mod_file)\n    \n  new_modis_pd = None\n  \n  for filename in os.listdir(mod_testing_folder):\n    f = os.path.join(mod_testing_folder, filename)\n    if os.path.isfile(f) and \".csv\" in f:\n      print(f)\n      old_modis_pd = pd.read_csv(f, header = 0)\n      old_modis_pd = old_modis_pd.drop(columns=['date'])\n      old_modis_pd.rename(columns = {'Unnamed: 0':'date'}, inplace = True)\n      #cell_id_list = old_modis_pd[\"cell_id\"].unique()\n      #cell_id_list = np.insert(cell_id_list, 0, \"data\")\n      cell_id_list = submission_format_pd.index\n      date_list = pd.date_range(start=start_date, end=end_date, freq='D').astype(str)\n\n      rows = date_list\n      cols = cell_id_list\n      \n      if new_modis_pd is None:\n        new_modis_pd = pd.DataFrame(([0.0 for col in cols] for row in rows), index=rows, columns=cols)\n      \n      for i, row in old_modis_pd.iterrows():\n        cdate = row['date']\n        ndsi = row['mod10a1_ndsi']\n        cellid = row['cell_id']\n        #print(f\"{cdate} - {ndsi} - {cellid}\")\n        if ndsi != 0:\n           new_modis_pd.at[cdate, cellid] = ndsi\n  \n  #modis_np = numpy.zeros((len(date_list), len(cell_id_list)+1))\n  #modis_np[0] = cell_id_list\n  \n  #s1_pd.loc[:, ~s1_pd.columns.str.match('Unnamed')]\n  #print(new_modis_pd.head())\n  new_modis_pd.to_csv(all_mod_file)\n\n  \ndef integrate_sentinel1():\n  \"\"\"\n  Integrate all Sentinel 1 data into sentinel1_all.csv\n  Turn the rows into \"daily\", right now it has datetime stamps.\n  \"\"\"\n  all_sentinel1_file = f\"{github_dir}/data/ready_for_testing/sentinel1_all.csv\"\n  ready_sentinel1_file = f\"{github_dir}/data/sat_testing/sentinel1/\"\n  if os.path.exists(all_sentinel1_file):\n    os.remove(all_sentinel1_file)\n  new_s1_pd = None\n  for filename in os.listdir(ready_sentinel1_file):\n    f = os.path.join(ready_sentinel1_file, filename)\n    if os.path.isfile(f) and \".csv\" in f:\n      print(f)\n      old_s1_pd = pd.read_csv(f, header = 0)\n      old_s1_pd = old_s1_pd.drop(columns=['date'])\n      old_s1_pd.rename(columns = {'Unnamed: 0':'date'}, inplace = True)\n      #s1_pd.loc[:, ~s1_pd.columns.str.match('Unnamed')]\n\n      #cell_id_list = old_s1_pd[\"cell_id\"].unique()\n      cell_id_list = submission_format_pd.index\n      #date_list = old_s1_pd[\"date\"].unique()\n      date_list = pd.date_range(start=start_date, end=end_date, freq='D').astype(str)\n      rows = date_list\n      cols = cell_id_list\n      \n      if new_s1_pd is None:\n        new_s1_pd = pd.DataFrame(([0.0 for col in cols] for row in rows), index=rows, columns=cols)\n\n      for i, row in old_s1_pd.iterrows():\n        cdate = row['date']\n        xdate = dt.strptime(cdate, \"%Y-%m-%d %H:%M:%S\") #3/7/2022  2:00:48 AM\n        sdate = xdate.strftime(\"%Y-%m-%d\")\n        grd = row['s1_grd_vv']\n        cellid = row['cell_id']\n        if grd == 0:\n          continue\n        new_s1_pd.at[sdate, cellid] = float(grd)\n  \n  new_s1_pd.to_csv(all_sentinel1_file)\n\ndef integrate_gridmet():\n  \"\"\"\n  Integrate all gridMET data into gridmet_all.csv\n  \"\"\"\n  \n  dates = pd.date_range(start=start_date, end=end_date, freq='D').astype(str)\n  \n  #print(mod_all_df.head())\n  var_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n  \n  for var in var_list:\n    print(\"Processing \", var)\n    all_single_var_file = f\"{github_dir}/data/ready_for_testing/gridmet_{var}_all.csv\"\n    \n    all_gridmet_var_folder = f\"{github_dir}/data/sim_testing/gridmet/\"\n    new_var_pd = None\n    \n    for filename in os.listdir(all_gridmet_var_folder):\n      f = os.path.join(all_gridmet_var_folder, filename)\n      if os.path.isfile(f) and \".csv\" in f:\n        print(f)\n        all_gridmet_var_pd = pd.read_csv(f, header=0)\n        #cell_id_list = old_s1_pd[\"cell_id\"].unique()\n        cell_id_list = submission_format_pd.index\n        #date_list = old_s1_pd[\"date\"].unique()\n        date_list = pd.date_range(start=start_date, end=end_date, freq='D').astype(str)\n        rows = date_list\n        cols = cell_id_list\n        if new_var_pd is None:\n          new_var_pd = pd.DataFrame(([0.0 for col in cols] for row in rows), index=rows, columns=cols)\n\n        for i, row in all_gridmet_var_pd.iterrows():\n          cdate = row[\"Unnamed: 0\"]\n          xdate = dt.strptime(cdate, \"%Y-%m-%d %H:%M:%S\") #3/7/2022  2:00:48 AM\n          sdate = xdate.strftime(\"%Y-%m-%d\")\n          newval = row[var]\n          cellid = row['cell_id']\n          if newval != 0:\n            new_var_pd.at[sdate, cellid] = float(newval)\n  \n    new_var_pd.to_csv(all_single_var_file)\n  \n  \ndef prepare_testing_csv():\n  \"\"\"\n  MOD model:\n    input columns: [m, doy, ndsi]\n    output column: [swe]\n  Sentinel1 model:\n    input columns: [m, doy, grd]\n    output column: [swe]\n  gridMET model:\n    input columns: [m, doy, tmmn, tmmx, pr, vpd, eto, rmax, rmin, vs]\n    output column: [swe]\n  \"\"\"\n  all_ready_file = f\"{github_dir}/data/ready_for_testing/all_ready_3.csv\"\n  if os.path.exists(all_ready_file):\n    os.remove(all_ready_file)\n  \n  all_mod_file = f\"{github_dir}/data/ready_for_testing/modis_all.csv\"\n  modis_all_pd = pd.read_csv(all_mod_file, header=0, index_col = 0)\n  modis_all_np = modis_all_pd.to_numpy()\n  \n  all_sentinel1_file = f\"{github_dir}/data/ready_for_testing/sentinel1_all.csv\"\n  sentinel1_all_pd = pd.read_csv(all_sentinel1_file, header=0, index_col = 0)\n  sentinel1_all_np = sentinel1_all_pd.to_numpy()\n  \n  all_gridmet_eto_file = f\"{github_dir}/data/ready_for_testing/gridmet_eto_all.csv\"\n  gridmet_eto_all_pd = pd.read_csv(all_gridmet_eto_file, header=0, index_col = 0)\n  gridmet_eto_all_np = gridmet_eto_all_pd.to_numpy()\n  \n  all_gridmet_pr_file = f\"{github_dir}/data/ready_for_testing/gridmet_pr_all.csv\"\n  gridmet_pr_all_pd = pd.read_csv(all_gridmet_pr_file, header=0, index_col = 0)\n  gridmet_pr_all_np = gridmet_pr_all_pd.to_numpy()\n  \n  all_gridmet_rmax_file = f\"{github_dir}/data/ready_for_testing/gridmet_rmax_all.csv\"\n  gridmet_rmax_all_pd = pd.read_csv(all_gridmet_rmax_file, header=0, index_col = 0)\n  gridmet_rmax_all_np = gridmet_rmax_all_pd.to_numpy()\n  \n  all_gridmet_rmin_file = f\"{github_dir}/data/ready_for_testing/gridmet_rmin_all.csv\"\n  gridmet_rmin_all_pd = pd.read_csv(all_gridmet_rmin_file, header=0, index_col = 0)\n  gridmet_rmin_all_np = gridmet_rmin_all_pd.to_numpy()\n  \n  all_gridmet_tmmn_file = f\"{github_dir}/data/ready_for_testing/gridmet_tmmn_all.csv\"\n  gridmet_tmmn_all_pd = pd.read_csv(all_gridmet_tmmn_file, header=0, index_col = 0)\n  gridmet_tmmn_all_np = gridmet_tmmn_all_pd.to_numpy()\n  \n  all_gridmet_tmmx_file = f\"{github_dir}/data/ready_for_testing/gridmet_tmmx_all.csv\"\n  gridmet_tmmx_all_pd = pd.read_csv(all_gridmet_tmmx_file, header=0, index_col = 0)\n  gridmet_tmmx_all_np = gridmet_tmmx_all_pd.to_numpy()\n  \n  all_gridmet_vpd_file = f\"{github_dir}/data/ready_for_testing/gridmet_vpd_all.csv\"\n  gridmet_vpd_all_pd = pd.read_csv(all_gridmet_vpd_file, header=0, index_col = 0)\n  gridmet_vpd_all_np = gridmet_vpd_all_pd.to_numpy()\n  \n  all_gridmet_vs_file = f\"{github_dir}/data/ready_for_testing/gridmet_vs_all.csv\"\n  gridmet_vs_all_pd = pd.read_csv(all_gridmet_vs_file, header=0, index_col = 0)\n  gridmet_vs_all_np = gridmet_vs_all_pd.to_numpy()\n  \n  grid_terrain_file = f\"{github_dir}/data/terrain/gridcells_eval_terrainData.csv\"\n  grid_terrain_pd = pd.read_csv(grid_terrain_file, header=0, index_col = 0)\n  grid_terrain_np = grid_terrain_pd.to_numpy()\n  \n  sentinel1_all_pd = sentinel1_all_pd[:modis_all_pd.shape[0]]\n  \n  \n  \n  print(\"modis_all_size: \", modis_all_pd.shape)\n  print(\"sentinel1_all_size: \", sentinel1_all_pd.shape)\n  print(\"gridmet rmax size: \", gridmet_rmax_all_pd.shape)\n  print(\"gridmet eto size: \", gridmet_eto_all_pd.shape)\n  print(\"gridmet vpd size: \", gridmet_vpd_all_pd.shape)\n  print(\"gridmet pr size: \", gridmet_pr_all_pd.shape)\n  print(\"gridmet rmin size: \", gridmet_rmin_all_pd.shape)\n  print(\"gridmet tmmn size: \", gridmet_tmmn_all_pd.shape)\n  print(\"gridmet tmmx size: \", gridmet_tmmx_all_pd.shape)\n  print(\"gridmet vs size: \", gridmet_vs_all_pd.shape)\n  print(\"grid terrain size: \", grid_terrain_pd.shape)\n  print(\"cell_size: \", len(submission_format_pd.index))\n  print(\"station size: \", station_cell_mapper_pd.shape)\n  print(\"training_feature_pd size: \", training_feature_pd.shape)\n  print(\"testing_feature_pd size: \", testing_feature_pd.shape)\n  print(\"grid_terrain_np shape: \", grid_terrain_np.shape)\n  \n  min_len = min( modis_all_pd.shape[0], sentinel1_all_pd.shape[0], gridmet_rmax_all_pd.shape[0], gridmet_eto_all_pd.shape[0], gridmet_vpd_all_pd.shape[0], gridmet_pr_all_pd.shape[0], gridmet_rmin_all_pd.shape[0], gridmet_tmmn_all_pd.shape[0], gridmet_tmmx_all_pd.shape[0], gridmet_vs_all_pd.shape[0], grid_terrain_pd.shape[0] )\n  \n  cell_id_list = modis_all_pd.columns.values\n  \n  \n  \n  # create a multiple numpy array, the dimension is (cell_id, date, variable)\n  #all_testing_np = np.empty((len(modis_all_pd.index.values), len(modis_all_pd.columns.values),  23))\n  all_testing_np = np.empty((min_len, len(modis_all_pd.columns.values),  23))\n  print(\"final all numpy shape: \", all_testing_np.shape)\n  \n  modis_all_np = np.expand_dims(modis_all_np[:min_len, :], axis=2)\n  sentinel1_all_np = np.expand_dims(sentinel1_all_np[:min_len, :], axis=2)\n  gridmet_eto_all_np = np.expand_dims(gridmet_eto_all_np[:min_len, :], axis=2)\n  gridmet_pr_all_np = np.expand_dims(gridmet_pr_all_np[:min_len, :], axis=2)\n  gridmet_rmax_all_np = np.expand_dims(gridmet_rmax_all_np[:min_len, :], axis=2)\n  gridmet_rmin_all_np = np.expand_dims(gridmet_rmin_all_np[:min_len, :], axis=2)\n  gridmet_tmmn_all_np = np.expand_dims(gridmet_tmmn_all_np[:min_len, :], axis=2)\n  gridmet_tmmx_all_np = np.expand_dims(gridmet_tmmx_all_np[:min_len, :], axis=2)\n  gridmet_vpd_all_np = np.expand_dims(gridmet_vpd_all_np[:min_len, :], axis=2)\n  gridmet_vs_all_np = np.expand_dims(gridmet_vs_all_np[:min_len, :], axis=2)\n  \n  cell_id_np = np.expand_dims(cell_id_list, axis=0)\n  cell_id_np = np.repeat(cell_id_np, min_len, axis=0)\n  cell_id_np = np.expand_dims(cell_id_np, axis=2)\n  print(\"cell_id_np shape: \", cell_id_np.shape)\n  \n  grid_terrain_np = np.expand_dims(grid_terrain_np, axis=0)\n  grid_terrain_np = np.repeat(grid_terrain_np, min_len, axis=0)\n  \n  date_np = np.empty((min_len, len(modis_all_pd.columns.values),  3))\n  for i in range(min_len):\n    #print(i, \" - \", modis_all_pd.index.values[i])\n    date_time_obj = dt.strptime(modis_all_pd.index.values[i], '%Y-%m-%d')\n    date_np[i, :, 0] = date_time_obj.year\n    date_np[i, :, 1] = date_time_obj.month\n    date_np[i, :, 2] = date_time_obj.timetuple().tm_yday\n  \n  new_np = np.concatenate((cell_id_np, date_np, modis_all_np, sentinel1_all_np, gridmet_eto_all_np, gridmet_pr_all_np, gridmet_rmax_all_np, gridmet_rmin_all_np, gridmet_tmmn_all_np, gridmet_tmmx_all_np, gridmet_vpd_all_np, gridmet_vs_all_np, grid_terrain_np), axis=2)\n  print(\"new numpy shape: \", new_np.shape)\n  \n  new_np = new_np.reshape(-1,new_np.shape[-1])\n  print(\"reshaped: \", new_np.shape)\n  \n  #all_training_pd = pd.DataFrame(columns=[\"cell_id\", \"year\", \"m\", \"doy\", \"ndsi\", \"grd\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"lat\", \"lon\", \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\", \"swe\"])\n  all_testing_pd = pd.DataFrame(new_np, columns=[\"cell_id\", \"year\", \"m\", \"doy\", \"ndsi\", \"grd\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"lat\", \"lon\", \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\"])\n  \n  #print(\"MODIS all np shape: \", modis_all_np.shape)\n  #print(\"Terrain numpy shape: \", grid_terrain_np.shape)\n  \n  #print(\"Head\", all_testing_pd.head())\n  all_testing_pd.to_csv(all_ready_file)\n  \n  \n  \n#exit() # done already\n\nintegrate_modis()\nintegrate_sentinel1()\nintegrate_gridmet()\nprepare_testing_csv()\n\n\n  \n  \n  \n",
  "history_output" : "C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py:148: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n  from . import _distributor_init\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\zcl43kjvatv\\testing_data_integration.py\", line 3, in <module>\n    import pandas as pd\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\pandas\\__init__.py\", line 16, in <module>\n    raise ImportError(\nImportError: Unable to import required dependencies:\nnumpy: \nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\nWe have compiled some common reasons and troubleshooting tips at:\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\nPlease note and check the following:\n  * The Python version is: Python3.9 from \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\python.exe\"\n  * The NumPy version is: \"1.21.5\"\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
  "history_begin_time" : 1667410697055,
  "history_end_time" : 1667410697177,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "m9a3a2mzmnl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696440,
  "history_end_time" : 1667410696440,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8ldghk8m6tq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696441,
  "history_end_time" : 1667410696441,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qxbbu7jdrxe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696441,
  "history_end_time" : 1667410696441,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l8d70y4rc85",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696442,
  "history_end_time" : 1667410696442,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8bi5n12jlfj",
  "history_input" : "import os\n\nprint(\"get ucla data and stuff\")\n#https://nsidc.org/data/wus_ucla_sr/versions/1",
  "history_output" : "get ucla data and stuff\n",
  "history_begin_time" : 1667410697922,
  "history_end_time" : 1667410698004,
  "history_notes" : null,
  "history_process" : "3r5rpn",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "gwwqsx1wjxz",
  "history_input" : "import os\n\nprint(\"get 4km SWE from nsidc or data storage)\n      \n#https://nsidc.org/data/nsidc-0719/versions/1",
  "history_output" : "  File \"C:\\Users\\BLi\\gw-workspace\\gwwqsx1wjxz\\data_nsidc_4km_swe.py\", line 3\n    print(\"get 4km SWE from nsidc or data storage)\n                                                  ^\nSyntaxError: EOL while scanning string literal\n",
  "history_begin_time" : 1667410698190,
  "history_end_time" : 1667410698272,
  "history_notes" : null,
  "history_process" : "0tdceb",
  "host_id" : "100001",
  "indicator" : "Failed"
}]
