[{
  "history_id" : "lg874k7eayk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667434821997,
  "history_end_time" : 1667434821997,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tzoksg4rjb4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667420912884,
  "history_end_time" : 1667420912884,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "okaorolnnuw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667420799806,
  "history_end_time" : 1667420799806,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qby3j50ycr5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667412122217,
  "history_end_time" : 1667412122217,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "40hze3jki4z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667411534156,
  "history_end_time" : 1667411534156,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cq6yoh38geg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667411146178,
  "history_end_time" : 1667411146178,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cdyi94kc2kn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410736839,
  "history_end_time" : 1667410736839,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qxbbu7jdrxe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696441,
  "history_end_time" : 1667410696441,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yfi5riiy4qm",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py:148: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n  from . import _distributor_init\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\yfi5riiy4qm\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\metloom\\pointdata\\__init__.py\", line 1, in <module>\n    from .base import PointData, PointDataCollection\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\metloom\\pointdata\\base.py\", line 5, in <module>\n    import pandas as pd\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\pandas\\__init__.py\", line 16, in <module>\n    raise ImportError(\nImportError: Unable to import required dependencies:\nnumpy: \nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\nWe have compiled some common reasons and troubleshooting tips at:\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\nPlease note and check the following:\n  * The Python version is: Python3.9 from \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\python.exe\"\n  * The NumPy version is: \"1.21.5\"\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
  "history_begin_time" : 1667410663645,
  "history_end_time" : 1667410690576,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pdm9ahgf1mh",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "['/Users/joe/gw-workspace/pdm9ahgf1mh', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper']\nhttps://www.nohrsc.noaa.gov/nearest/index.html?city=40.05352381745094%2C-106.04027196859343&county=&l=5&u=e&y=2022&m=5&d=4\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\" >\n<html lang=\"en\">\n<head>\n\t<meta http-equiv=\"Content-Type\" CONTENT=\"text/html; charset=utf-8\" >\n\t<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/main.css\" >\n\t<link href=\"/favicon.ico\" rel=\"shortcut icon\" >\n\t<meta http-equiv=\"Cache-Control\" content=\"no-cache\" >\n\t<meta name=\"DC.creator\" content=\"National Operational Hydrologic Remote Sensing Center\" >\n\t<meta name=\"DC.publisher\" content=\"NOAA's National Weather Service\" >\n\t<meta name=\"DC.contributor\" content=\"National Operational Hydrologic Remote Sensing Center\" >\n\t<meta name=\"DC.language\" content=\"EN-US\" >\n<title>Nearest Observations - NOHRSC - The ultimate source for snow information</title>\n<meta name=\"DC.title\" content=\"Nearest Observations - NOHRSC - The ultimate source for snow information\">\n<meta name=\"DC.description\" content=\"A listing of nearby observations to a given point and date\">\n<meta name=\"DC.date.created\" scheme=\"ISO8601\" content=\"2009-01-12\">\n<meta name=\"DC.date.reviewed\" scheme=\"ISO8601\" content=\"2019-11-20\">\n\t<script type=\"text/javascript\">\n\t</script>\n\t<!-- Global site tag (gtag.js) - Google Analytics -->\n\t<script type=\"text/javascript\" async src=\"https://www.googletagmanager.com/gtag/js?id=UA-43953030-10\"></script>\n\t<script type=\"text/javascript\">\n\t  window.dataLayer = window.dataLayer || [];\n\t  function gtag(){dataLayer.push(arguments);}\n\t  gtag('js', new Date());\n\t  gtag('config', 'UA-43953030-10');\n\t</script>\n</head>\n<body>\n\t<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" width=\"100%\" style=\"background-image : url(/images/topbanner.jpg)\">\n\t\t<tr>\n\t\t\t<td align=\"right\" height=\"19\">\n\t\t\t\t<a href=\"#content\"><img src=\"/images/skipgraphic.gif\" alt=\"(content link)\" height=\"1\" width=\"1\" border=\"0\"></a>\n\t\t\t\t<a href=\"https://www.nws.noaa.gov\"><span class=\"nwslink\">weather.gov</span></a>\n\t\t\t\t&nbsp;&nbsp;&nbsp;\n\t\t\t</td>\n\t\t</tr>\n\t</table>\n\t<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" width=\"100%\">\n\t\t<tr>\n\t\t\t<td rowspan=\"2\" width=\"85\"><a href=\"https://www.noaa.gov\"><img src=\"/images/titleleft_noaa.jpg\" alt=\"NOAA link\" width=\"85\" height=\"78\" border=\"0\"></a></td>\n\t\t\t<td align=\"left\" width=\"500\" height=\"20\" style=\"background : url(/images/blank_title.jpg);\"><div class=\"source\">National Weather Service</div></td>\n\t\t\t<td rowspan=\"2\" style=\"background-image : url(/images/title_bg.jpg)\">&nbsp;</td>\n\t\t\t<td rowspan=\"2\" width=\"85\" align=\"right\"><a href=\"https://www.nws.noaa.gov\"><img src=\"/images/titleright_nws.jpg\" alt=\"NWS link\" width=\"85\" height=\"78\" border=\"0\"></a></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"500\" height=\"58\" style=\"background : url(/images/blank_name.jpg);\" class=\"location\"><a href=\"/\">National Operational Hydrologic<br> Remote Sensing Center</a></td>\n\t\t</tr>\n\t</table>\n\t<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" style=\"background-image : url(/images/navbar_bg.gif)\" width=\"100%\" class=\"nav\">\n\t\t<tr>\n\t\t\t<td align=\"left\" valign=\"top\" width=\"94\"><img src=\"/images/navbar_left.jpg\" alt=\"\" width=\"94\" height=\"23\" border=\"0\"></td>\n\t\t\t<td class=\"nav\" width=\"15%\" align=\"center\" nowrap><a href=\"/\">Home</a></td>\n\t\t\t<td class=\"nav\" width=\"15%\" align=\"center\"><a href=\"https://www.weather.gov/news\" title=\"National Weather Service News\">News</a></td>\n\t\t\t<td class=\"nav\" width=\"20%\" align=\"center\"><a href=\"https://www.nws.noaa.gov/organization.html\" title=\"National Weather Service Organization\">Organization</a></td>\n\t\t\t<td align=\"left\" class=\"searchinput\" width=\"20%\" nowrap=\"nowrap\">\n\t\t\t\t<form action=\"https://search.usa.gov/search\" method=\"get\" name=\"query\" id=\"query\"\n\t\t\t\t style=\"margin-bottom:0; margin-top:0;\">\n\t\t\t\t<label for=\"search\" class=\"yellow\">Search</label>&nbsp;&nbsp;\n\t\t\t\t<input type=\"hidden\" name=\"affiliate\" value=\"nws.noaa.gov\" >\n\t\t\t\t<input type=\"text\" name=\"query\" id=\"search\" value=\"Enter Search Here\"\n\t\t\t\t size=\"20\" maxlength=\"256\" onfocus=\"this.value='';\" title=\"Search all NWS sites here\">&nbsp;\n\t\t\t\t<input type=\"submit\" id=\"submit\" value=\"Go\" >\n\t\t\t\t</form>\n\t\t\t</td>\n\t\t\t<td width=\"10%\">&nbsp;</td>\n\t\t\t<td align=\"right\" valign=\"bottom\" width=\"24\"><img src=\"/images/navbar_right.jpg\" alt=\"\" width=\"24\" height=\"23\" border=\"0\"></td>\n\t\t</tr>\n\t</table>\n\t<table cellspacing=\"0\" cellpadding=\"0\">\n\t\t<tr valign=\"top\">\n\t\t\t<td class=\"nav\" width=\"130\">\n<dl>\n<dd><a href=\"/\">Home</a></dd>\n</dl>\n<dl>\n<dt>Snow Information</dt>\n<dd><a href=\"/nsa/\" title=\"An overview of snow around the country\">National Analyses</a></dd>\n<dd><a href=\"/interactive/html/map.html\" title=\"Explore NOHRSC products and create your own maps\">Interactive Maps</a></dd>\n<dd><a href=\"/earth/\" title=\"A listing of experimental products for use with KML interpreter software\">3D Visualization</a></dd>\n<dd><a href=\"/snowsurvey/\" title=\"Current information about our snow surveys\">Airborne Surveys</a></dd>\n<dd><a href=\"/snowfall/\" title=\"Daily maps of observed snowfall\">Snowfall Analysis</a></dd>\n<dd><a href=\"/nh_snowcover/\">Satellite Products</a></dd>\n<dd><a href=\"/forecasts/\">Forecasts</a></dd>\n<dd><a href=\"/archived_data/\" title=\"Information on where to acquire NOHRSC raster data\">Data Archive</a></dd>\n<dd><a href=\"/shef_archive/\" title=\"Browse an archive of SHEF text messages\">SHEF Products</a></dd>\n</dl>\n<form name=\"nearest\" action=\"/nearest/index.html\">\n<dl>\n<dt>Observations near</dt>\n<dd><input type=\"text\" size=\"9\" name=\"city\" value=\"City, ST\" title=\"Search for snow observations near a city. Press enter or select the go button to submit request\" onfocus=\"this.value='';\">\n<input type=\"submit\" value=\"Go\"></dd>\n</dl>\n</form>\n<dl>\n<dt>Science/Technology</dt>\n<dd><a href=\"/technology/\" title=\"More detailed information about the NOHRSC\">NOHRSC</a></dd>\n<dd><a href=\"/gisdatasets/\" title=\"Shapefiles available for download\">GIS Data Sets</a></dd>\n<dd><a href=\"/special/\" title=\"Satellite/GIS images for certain projects\">Special Purpose Imagery</a></dd>\n</dl>\n<dl>\n<dt>About The NOHRSC</dt>\n<dd><a href=\"/directory/\" title=\"Meet the staff at the NOHRSC\">Staff</a></dd>\n</dl>\n<dl>\n<dt>NOAA Links</dt>\n<dd><a href=\"https://www.ncdc.noaa.gov/snow-and-ice/\">Snow Climatology</a></dd>\n<dd><a href=\"/links.html\">Related Links</a></dd>\n</dl>\n<dl>\n<dt>Help</dt>\n<dd><a href=\"/help/\" title=\"NOHRSC web site help\">Help and FAQ</a></dd>\n<dd><a href=\"/sitemap.html\">Site Map</a></dd>\n</dl>\n<dl>\n<dt>Contact Us</dt>\n<dd><a href=\"/contact.html\">Please Send Us Comments!</a></dd>\n</dl>\n<center>\n<a href=\"https://www.usa.gov\"><img src=\"/images/usagov_logo_color_110w.gif\" alt=\"USA.gov is the U.S. Government's official Web portal to all Federal, state and local government Web resources and services.\" width=\"110\" height=\"30\" border=\"0\"></a>\n<br>\n</center>\n\t\t\t</td>\n\t\t\t<td id=\"content_block\">\n\t\t\t\t<a href=\"\" name=\"content\"></a>\n<center><strong>Nearest observations to</strong></center>\n<center><h2>40.05&deg;N, -106.04&deg;W</h2></center>\n<strong>Note: these data are unofficial and provisional.</strong><br>\n<form name=\"data\" action=\"/nearest/index.html\">\n<fieldset>\n<legend><b>Location and Date</b></legend>\n<label title=\"Enter City, ST (or Latitude, Longitude)\">Enter your \"City, ST (or Latitude, Longitude)\" <input type=\"text\" size=\"16\" maxlength=\"80\" name=\"city\" value=\"40.05&deg;N, -106.04&deg;W\"></label>\n<input type=\"hidden\" name=\"county\" value=\"\">\n<input type=\"submit\" value=\"Go\" title=\"Click to refresh screen\"><br><br>\n<input type=\"hidden\" name=\"l\" value=\"5\">\n<select name=\"u\" title=\"Units\" size=\"1\" class=\"smallform\">\n<option value=\"e\" class=\"smallform\" selected>English</option>\n<option value=\"m\" class=\"smallform\">Metric</option>\n</select>\n &nbsp;\n<select name=\"y\" title=\"Year\" size=\"1\" class=\"smallform\">\n<option value=\"2003\" class=\"smallform\">2003</option>\n<option value=\"2004\" class=\"smallform\">2004</option>\n<option value=\"2005\" class=\"smallform\">2005</option>\n<option value=\"2006\" class=\"smallform\">2006</option>\n<option value=\"2007\" class=\"smallform\">2007</option>\n<option value=\"2008\" class=\"smallform\">2008</option>\n<option value=\"2009\" class=\"smallform\">2009</option>\n<option value=\"2010\" class=\"smallform\">2010</option>\n<option value=\"2011\" class=\"smallform\">2011</option>\n<option value=\"2012\" class=\"smallform\">2012</option>\n<option value=\"2013\" class=\"smallform\">2013</option>\n<option value=\"2014\" class=\"smallform\">2014</option>\n<option value=\"2015\" class=\"smallform\">2015</option>\n<option value=\"2016\" class=\"smallform\">2016</option>\n<option value=\"2017\" class=\"smallform\">2017</option>\n<option value=\"2018\" class=\"smallform\">2018</option>\n<option value=\"2019\" class=\"smallform\">2019</option>\n<option value=\"2020\" class=\"smallform\">2020</option>\n<option value=\"2021\" class=\"smallform\">2021</option>\n<option value=\"2022\" class=\"smallform\" selected>2022</option>\n</select>\n &nbsp;\n<select name=\"m\" title=\"Month\" size=\"1\" class=\"smallform\">\n<option value=\"1\" class=\"smallform\">January</option>\n<option value=\"2\" class=\"smallform\">February</option>\n<option value=\"3\" class=\"smallform\">March</option>\n<option value=\"4\" class=\"smallform\">April</option>\n<option value=\"5\" class=\"smallform\" selected>May</option>\n<option value=\"6\" class=\"smallform\">June</option>\n<option value=\"7\" class=\"smallform\">July</option>\n<option value=\"8\" class=\"smallform\">August</option>\n<option value=\"9\" class=\"smallform\">September</option>\n<option value=\"10\" class=\"smallform\">October</option>\n<option value=\"11\" class=\"smallform\">November</option>\n<option value=\"12\" class=\"smallform\">December</option>\n</select>\n &nbsp;\n<select name=\"d\" title=\"Day\" size=\"1\" class=\"smallform\">\n<option value=\"1\" class=\"smallform\">1</option>\n<option value=\"2\" class=\"smallform\">2</option>\n<option value=\"3\" class=\"smallform\">3</option>\n<option value=\"4\" class=\"smallform\" selected>4</option>\n<option value=\"5\" class=\"smallform\">5</option>\n<option value=\"6\" class=\"smallform\">6</option>\n<option value=\"7\" class=\"smallform\">7</option>\n<option value=\"8\" class=\"smallform\">8</option>\n<option value=\"9\" class=\"smallform\">9</option>\n<option value=\"10\" class=\"smallform\">10</option>\n<option value=\"11\" class=\"smallform\">11</option>\n<option value=\"12\" class=\"smallform\">12</option>\n<option value=\"13\" class=\"smallform\">13</option>\n<option value=\"14\" class=\"smallform\">14</option>\n<option value=\"15\" class=\"smallform\">15</option>\n<option value=\"16\" class=\"smallform\">16</option>\n<option value=\"17\" class=\"smallform\">17</option>\n<option value=\"18\" class=\"smallform\">18</option>\n<option value=\"19\" class=\"smallform\">19</option>\n<option value=\"20\" class=\"smallform\">20</option>\n<option value=\"21\" class=\"smallform\">21</option>\n<option value=\"22\" class=\"smallform\">22</option>\n<option value=\"23\" class=\"smallform\">23</option>\n<option value=\"24\" class=\"smallform\">24</option>\n<option value=\"25\" class=\"smallform\">25</option>\n<option value=\"26\" class=\"smallform\">26</option>\n<option value=\"27\" class=\"smallform\">27</option>\n<option value=\"28\" class=\"smallform\">28</option>\n<option value=\"29\" class=\"smallform\">29</option>\n<option value=\"30\" class=\"smallform\">30</option>\n<option value=\"31\" class=\"smallform\">31</option>\n</select>\n &nbsp;\n<input type=\"submit\" name=\"i\" value=\" - \" title=\"Back one day\"> &nbsp;\n<input type=\"submit\" name=\"i\" value=\" + \" title=\"Forward one day\"> &nbsp;\n</fieldset>\n</form>\n<table width=\"100%\"><tr><td><strong>Closest 5  observations near 40.05&deg;N, -106.04&deg;W</strong><br>40.05&deg;N, -106.04&deg;W (Elevation: N/A)</td>\n<td align=\"right\">Latest between <span class=\"date\">2022-05-04 06:00 UTC</span><br>and  <span class=\"date\">2022-05-05 06:00 UTC</span></td></tr></table><hr>\n<table class=\"gray_data_table\" cellspacing=\"2\" summary=\"Table of Raw Snowfall Observations near 40.05&deg;N, -106.04&deg;W, \" width=\"100%\">\n<caption><strong>Raw Snowfall Observations</strong></caption>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <thead>\n  <tr>\n<th scope=\"col\">Station ID</th><th scope=\"col\">Name</th><th scope=\"col\">Elev.<br> (ft)</th><th scope=\"col\">Raw Snowfall<br> (in)</th><th scope=\"col\">Duration<br> (hours)</th><th scope=\"col\">Date (UTC)</th><th scope=\"col\">Distance</th></tr>\n </thead>\n <tbody>\n<tr><td class=\"desc\" title=\"(40.0071&deg;N, -105.8862&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-68\">CO-GR-68</a></td><td class=\"desc\">TABERNASH 2.7 NW, CO</td><td>8806</td><td>3.70</td><td>24</td><td>2022-05-04 13</td><td>8.7 mi ESE</td></tr>\n<tr><td class=\"desc\" title=\"(40.0375&deg;N, -106.203&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=WIFC2\">WIFC2</a></td><td class=\"desc\">WILLIAMS FORK DAM</td><td>7733</td><td>2.00</td><td>24</td><td>2022-05-04 14</td><td>8.7 mi W</td></tr>\n<tr><td class=\"desc\" title=\"(40.0911&deg;N, -106.2&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-52\">CO-GR-52</a></td><td class=\"desc\">PARSHALL 3.0 NNW, CO</td><td>7904</td><td>2.20</td><td>24</td><td>2022-05-04 13</td><td>8.9 mi WNW</td></tr>\n<tr><td class=\"desc\" title=\"(40.0015&deg;N, -105.8725&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-53\">CO-GR-53</a></td><td class=\"desc\">TABERNASH 1.9 NW, CO</td><td>8579</td><td>6.50</td><td>24</td><td>2022-05-04 13</td><td>9.5 mi ESE</td></tr>\n<tr><td class=\"desc\" title=\"(40.1997&deg;N, -105.9258&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-73\">CO-GR-73</a></td><td class=\"desc\">GRANBY 7.7 N, CO</td><td>8563</td><td>4.00</td><td>24</td><td>2022-05-04 13</td><td>12 mi NE</td></tr>\n</tbody></table>\n<hr>\n<table class=\"gray_data_table\" cellspacing=\"2\" summary=\"Table of Snow Depth Observations near 40.05&deg;N, -106.04&deg;W, \" width=\"100%\">\n<caption><strong>Snow Depth Observations</strong></caption>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <thead>\n  <tr>\n<th scope=\"col\">Station ID</th><th scope=\"col\">Name</th><th scope=\"col\">Elev.<br> (ft)</th><th scope=\"col\">Snow Depth<br> (in)</th><th scope=\"col\">Date (UTC)</th><th scope=\"col\">Distance</th></tr>\n </thead>\n <tbody>\n<tr><td class=\"desc\" title=\"(40.0071&deg;N, -105.8862&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-68\">CO-GR-68</a></td><td class=\"desc\">TABERNASH 2.7 NW, CO</td><td>8806</td><td>4.00</td><td>2022-05-04 13</td><td>8.7 mi ESE</td></tr>\n<tr><td class=\"desc\" title=\"(40.0375&deg;N, -106.203&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=WIFC2\">WIFC2</a></td><td class=\"desc\">WILLIAMS FORK DAM</td><td>7733</td><td>1.00</td><td>2022-05-04 14</td><td>8.7 mi W</td></tr>\n<tr><td class=\"desc\" title=\"(40.0911&deg;N, -106.2&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-52\">CO-GR-52</a></td><td class=\"desc\">PARSHALL 3.0 NNW, CO</td><td>7904</td><td>1.50</td><td>2022-05-04 13</td><td>8.9 mi WNW</td></tr>\n<tr><td class=\"desc\" title=\"(40.2254&deg;N, -105.9198&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=SCSC2\">SCSC2</a></td><td class=\"desc\">STILLWATER CREEK</td><td>8793</td><td>0.00</td><td>2022-05-05 05</td><td>13.7 mi NE</td></tr>\n<tr><td class=\"desc\" title=\"(40.2082&deg;N, -105.8634&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-78\">CO-GR-78</a></td><td class=\"desc\">GRAND LAKE 3.7 SW, CO</td><td>8537</td><td>3.00</td><td>2022-05-04 13</td><td>14.4 mi NE</td></tr>\n</tbody></table>\n<hr>\n<table class=\"gray_data_table\" cellspacing=\"2\" summary=\"Table of Snow Water Equivalent Observations near 40.05&deg;N, -106.04&deg;W, \" width=\"100%\">\n<caption><strong>Snow Water Equivalent Observations</strong></caption>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <thead>\n  <tr>\n<th scope=\"col\">Station ID</th><th scope=\"col\">Name</th><th scope=\"col\">Elev.<br> (ft)</th><th scope=\"col\">Snow Water Equivalent<br> (in)</th><th scope=\"col\">Date (UTC)</th><th scope=\"col\">Distance</th></tr>\n </thead>\n <tbody>\n<tr><td class=\"desc\" title=\"(40.2254&deg;N, -105.9198&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=SCSC2\">SCSC2</a></td><td class=\"desc\">STILLWATER CREEK</td><td>8793</td><td>0.00</td><td>2022-05-05 05</td><td>13.7 mi NE</td></tr>\n<tr><td class=\"desc\" title=\"(39.8687&deg;N, -105.8675&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=FCVC2\">FCVC2</a></td><td class=\"desc\">FOOL CREEK</td><td>11168</td><td>20.30</td><td>2022-05-04 07</td><td>15.5 mi SE</td></tr>\n<tr><td class=\"desc\" title=\"(39.7956&deg;N, -106.0273&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=MFKC2\">MFKC2</a></td><td class=\"desc\">MIDDLE FORK CAMP</td><td>8983</td><td>1.40</td><td>2022-05-05 05</td><td>17.6 mi S</td></tr>\n<tr><td class=\"desc\" title=\"(40.347&deg;N, -106.0943&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=WLLC2\">WLLC2</a></td><td class=\"desc\">WILLOW CREEK PASS</td><td>9600</td><td>14.70</td><td>2022-05-05 05</td><td>20.7 mi N</td></tr>\n<tr><td class=\"desc\" title=\"(39.7645&deg;N, -105.9062&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=JNPC2\">JNPC2</a></td><td class=\"desc\">JONES PASS</td><td>10482</td><td>10.80</td><td>2022-05-05 05</td><td>20.9 mi SSE</td></tr>\n</tbody></table>\n<hr>\n<table class=\"gray_data_table\" cellspacing=\"2\" summary=\"Table of Raw Precipitation Observations near 40.05&deg;N, -106.04&deg;W, \" width=\"100%\">\n<caption><strong>Raw Precipitation Observations</strong></caption>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <thead>\n  <tr>\n<th scope=\"col\">Station ID</th><th scope=\"col\">Name</th><th scope=\"col\">Elev.<br> (ft)</th><th scope=\"col\">Raw Precipitation<br> (in)</th><th scope=\"col\">Duration<br> (hours)</th><th scope=\"col\">Date (UTC)</th><th scope=\"col\">Distance</th></tr>\n </thead>\n <tbody>\n<tr><td class=\"desc\" title=\"(40.1083&deg;N, -106.0036&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CAWC2\">CAWC2</a></td><td class=\"desc\">COLORADO RVR BLW WINDY GAP</td><td>7822</td><td>0.23</td><td>24</td><td>2022-05-05 06</td><td>4.5 mi NNE</td></tr>\n<tr><td class=\"desc\" title=\"(40.1186&deg;N, -105.8997&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-81\">CO-GR-81</a></td><td class=\"desc\">GRANBY 2.9 NE, CO</td><td>8041</td><td>0.65</td><td>24</td><td>2022-05-05 01</td><td>8.8 mi ENE</td></tr>\n<tr><td class=\"desc\" title=\"(40.136&deg;N, -106.1744&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=GRUC2\">GRUC2</a></td><td class=\"desc\">GROUSE MOUNTAIN</td><td>10013</td><td>0.00</td><td>1</td><td>2022-05-05 05</td><td>9.3 mi WNW</td></tr>\n<tr><td class=\"desc\" title=\"(39.8906&deg;N, -106.0367&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=KSEC2\">KSEC2</a></td><td class=\"desc\">KEYSER RIDGE</td><td>10190</td><td>0.00</td><td>1</td><td>2022-05-05 05</td><td>11 mi S</td></tr>\n<tr><td class=\"desc\" title=\"(39.8906&deg;N, -106.0367&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=KSEC2\">KSEC2</a></td><td class=\"desc\">KEYSER RIDGE</td><td>10190</td><td>0.29</td><td>24</td><td>2022-05-05 05</td><td>11 mi S</td></tr>\n</tbody></table>\n<br>\n<span style=\"color: white\">Page generated in 0.00186 seconds.</span><br>\n\t\t\t\t<br><br>\n\t\t\t\t<table align=\"center\" cellspacing=\"2\" cellpadding=\"2\" border=\"0\">\n\t\t\t\t\t<tr align=\"center\">\n\t\t\t\t\t\t<td>\n\t\t\t\t\t\t\tNOHRSC<br>\n\t\t\t\t\t\t\t<a href=\"/mission.html\">Mission Statement</a>\n\t\t\t\t\t\t\t&nbsp;|&nbsp;\n\t\t\t\t\t\t\t<a href=\"/contact.html\">Contact</a>\n\t\t\t\t\t\t</td>\n\t\t\t\t\t</tr>\n\t\t\t\t</table>\n\t\t\t\t<table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td colspan=\"3\"><hr></td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr valign=\"top\"> \n\t\t\t\t\t\t<td align=\"left\" class=\"gray\">\n\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov\"><span class=\"gray\">National Weather Service</span></a><br>\n\t\t\t\t\t\t\tNational Operational Hydrologic Remote Sensing Center<br>\n                                                        <a href=\"https://water.noaa.gov\"><span class=\"gray\">Office of Water Prediction</span></a><br>\n\t\t\t\t\t\t\t1735 Lake Drive W.<br>\n\t\t\t\t\t\t\tChanhassen, MN 55317<br>\n\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t</td>\n\t\t\t\t\t\t<td align=\"right\">\n\t\t\t\t\t\t\t<a href=\"/\"><img src=\"/images/nohrsc.png\" alt=\"NOHRSC homepage\" border=0></a>\n\t\t\t\t\t\t</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr valign=\"top\">\n\t\t\t\t\t\t<td align=\"left\" class=\"gray\">\n\t\t\t\t\t\t\t<a href=\"/contact.html\"><span class=\"gray\">Contact NOHRSC</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/glossary/\"><span class=\"gray\">Glossary</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/credits.php\"><span class=\"gray\">Credits</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.cio.noaa.gov/Policy_Programs/info_quality.html\"><span class=\"gray\">Information Quality</span></a><br>\nPage last modified: Nov 20, 2019 <br>\n\t\t\t\t\t\t</td>\n\t\t\t\t\t\t<td align=\"right\" class=\"gray\">\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/admin.php\"><span class=\"gray\">About Us</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/disclaimer.php\"><span class=\"gray\">Disclaimer</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/privacy.php\"><span class=\"gray\">Privacy Policy</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.rdc.noaa.gov/~foia/\"><span class=\"gray\">FOIA</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/careers.php\"><span class=\"gray\">Career Opportunities</span></a><br>\n\t\t\t\t\t\t</td>\n\t\t\t\t\t</tr>\n\t\t\t\t</table>\n\t\t\t</td>\n\t\t</tr>\n\t</table>\n</body>\n</html>\n/Users/joe/gw-workspace/pdm9ahgf1mh/data_snotel_real_time.py:28: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\nThe code that caused this warning is on line 28 of the file /Users/joe/gw-workspace/pdm9ahgf1mh/data_snotel_real_time.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n  parsed_html = BeautifulSoup(webContent)\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/pdm9ahgf1mh/data_snotel_real_time.py\", line 29, in <module>\n    print(parsed_html.body.find('div', attrs={'class':'container'}).text)\nAttributeError: 'NoneType' object has no attribute 'text'\n",
  "history_begin_time" : 1667410652804,
  "history_end_time" : 1667410705881,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9kbsascoe6e",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py:148: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n  from . import _distributor_init\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\9kbsascoe6e\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\metloom\\pointdata\\__init__.py\", line 1, in <module>\n    from .base import PointData, PointDataCollection\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\metloom\\pointdata\\base.py\", line 5, in <module>\n    import pandas as pd\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\pandas\\__init__.py\", line 16, in <module>\n    raise ImportError(\nImportError: Unable to import required dependencies:\nnumpy: \nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\nWe have compiled some common reasons and troubleshooting tips at:\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\nPlease note and check the following:\n  * The Python version is: Python3.9 from \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\python.exe\"\n  * The NumPy version is: \"1.21.5\"\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
  "history_begin_time" : 1667410577695,
  "history_end_time" : 1667410651125,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "t5ck5kqz27u",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "['/Users/joe/gw-workspace/t5ck5kqz27u', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper']\nhttps://www.nohrsc.noaa.gov/nearest/index.html?city=40.05352381745094%2C-106.04027196859343&county=&l=5&u=e&y=2022&m=5&d=4\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\" >\n<html lang=\"en\">\n<head>\n\t<meta http-equiv=\"Content-Type\" CONTENT=\"text/html; charset=utf-8\" >\n\t<link rel=\"stylesheet\" type=\"text/css\" href=\"/css/main.css\" >\n\t<link href=\"/favicon.ico\" rel=\"shortcut icon\" >\n\t<meta http-equiv=\"Cache-Control\" content=\"no-cache\" >\n\t<meta name=\"DC.creator\" content=\"National Operational Hydrologic Remote Sensing Center\" >\n\t<meta name=\"DC.publisher\" content=\"NOAA's National Weather Service\" >\n\t<meta name=\"DC.contributor\" content=\"National Operational Hydrologic Remote Sensing Center\" >\n\t<meta name=\"DC.language\" content=\"EN-US\" >\n<title>Nearest Observations - NOHRSC - The ultimate source for snow information</title>\n<meta name=\"DC.title\" content=\"Nearest Observations - NOHRSC - The ultimate source for snow information\">\n<meta name=\"DC.description\" content=\"A listing of nearby observations to a given point and date\">\n<meta name=\"DC.date.created\" scheme=\"ISO8601\" content=\"2009-01-12\">\n<meta name=\"DC.date.reviewed\" scheme=\"ISO8601\" content=\"2019-11-20\">\n\t<script type=\"text/javascript\">\n\t</script>\n\t<!-- Global site tag (gtag.js) - Google Analytics -->\n\t<script type=\"text/javascript\" async src=\"https://www.googletagmanager.com/gtag/js?id=UA-43953030-10\"></script>\n\t<script type=\"text/javascript\">\n\t  window.dataLayer = window.dataLayer || [];\n\t  function gtag(){dataLayer.push(arguments);}\n\t  gtag('js', new Date());\n\t  gtag('config', 'UA-43953030-10');\n\t</script>\n</head>\n<body>\n\t<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" width=\"100%\" style=\"background-image : url(/images/topbanner.jpg)\">\n\t\t<tr>\n\t\t\t<td align=\"right\" height=\"19\">\n\t\t\t\t<a href=\"#content\"><img src=\"/images/skipgraphic.gif\" alt=\"(content link)\" height=\"1\" width=\"1\" border=\"0\"></a>\n\t\t\t\t<a href=\"https://www.nws.noaa.gov\"><span class=\"nwslink\">weather.gov</span></a>\n\t\t\t\t&nbsp;&nbsp;&nbsp;\n\t\t\t</td>\n\t\t</tr>\n\t</table>\n\t<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" width=\"100%\">\n\t\t<tr>\n\t\t\t<td rowspan=\"2\" width=\"85\"><a href=\"https://www.noaa.gov\"><img src=\"/images/titleleft_noaa.jpg\" alt=\"NOAA link\" width=\"85\" height=\"78\" border=\"0\"></a></td>\n\t\t\t<td align=\"left\" width=\"500\" height=\"20\" style=\"background : url(/images/blank_title.jpg);\"><div class=\"source\">National Weather Service</div></td>\n\t\t\t<td rowspan=\"2\" style=\"background-image : url(/images/title_bg.jpg)\">&nbsp;</td>\n\t\t\t<td rowspan=\"2\" width=\"85\" align=\"right\"><a href=\"https://www.nws.noaa.gov\"><img src=\"/images/titleright_nws.jpg\" alt=\"NWS link\" width=\"85\" height=\"78\" border=\"0\"></a></td>\n\t\t</tr>\n\t\t<tr>\n\t\t\t<td width=\"500\" height=\"58\" style=\"background : url(/images/blank_name.jpg);\" class=\"location\"><a href=\"/\">National Operational Hydrologic<br> Remote Sensing Center</a></td>\n\t\t</tr>\n\t</table>\n\t<table cellspacing=\"0\" cellpadding=\"0\" border=\"0\" style=\"background-image : url(/images/navbar_bg.gif)\" width=\"100%\" class=\"nav\">\n\t\t<tr>\n\t\t\t<td align=\"left\" valign=\"top\" width=\"94\"><img src=\"/images/navbar_left.jpg\" alt=\"\" width=\"94\" height=\"23\" border=\"0\"></td>\n\t\t\t<td class=\"nav\" width=\"15%\" align=\"center\" nowrap><a href=\"/\">Home</a></td>\n\t\t\t<td class=\"nav\" width=\"15%\" align=\"center\"><a href=\"https://www.weather.gov/news\" title=\"National Weather Service News\">News</a></td>\n\t\t\t<td class=\"nav\" width=\"20%\" align=\"center\"><a href=\"https://www.nws.noaa.gov/organization.html\" title=\"National Weather Service Organization\">Organization</a></td>\n\t\t\t<td align=\"left\" class=\"searchinput\" width=\"20%\" nowrap=\"nowrap\">\n\t\t\t\t<form action=\"https://search.usa.gov/search\" method=\"get\" name=\"query\" id=\"query\"\n\t\t\t\t style=\"margin-bottom:0; margin-top:0;\">\n\t\t\t\t<label for=\"search\" class=\"yellow\">Search</label>&nbsp;&nbsp;\n\t\t\t\t<input type=\"hidden\" name=\"affiliate\" value=\"nws.noaa.gov\" >\n\t\t\t\t<input type=\"text\" name=\"query\" id=\"search\" value=\"Enter Search Here\"\n\t\t\t\t size=\"20\" maxlength=\"256\" onfocus=\"this.value='';\" title=\"Search all NWS sites here\">&nbsp;\n\t\t\t\t<input type=\"submit\" id=\"submit\" value=\"Go\" >\n\t\t\t\t</form>\n\t\t\t</td>\n\t\t\t<td width=\"10%\">&nbsp;</td>\n\t\t\t<td align=\"right\" valign=\"bottom\" width=\"24\"><img src=\"/images/navbar_right.jpg\" alt=\"\" width=\"24\" height=\"23\" border=\"0\"></td>\n\t\t</tr>\n\t</table>\n\t<table cellspacing=\"0\" cellpadding=\"0\">\n\t\t<tr valign=\"top\">\n\t\t\t<td class=\"nav\" width=\"130\">\n<dl>\n<dd><a href=\"/\">Home</a></dd>\n</dl>\n<dl>\n<dt>Snow Information</dt>\n<dd><a href=\"/nsa/\" title=\"An overview of snow around the country\">National Analyses</a></dd>\n<dd><a href=\"/interactive/html/map.html\" title=\"Explore NOHRSC products and create your own maps\">Interactive Maps</a></dd>\n<dd><a href=\"/earth/\" title=\"A listing of experimental products for use with KML interpreter software\">3D Visualization</a></dd>\n<dd><a href=\"/snowsurvey/\" title=\"Current information about our snow surveys\">Airborne Surveys</a></dd>\n<dd><a href=\"/snowfall/\" title=\"Daily maps of observed snowfall\">Snowfall Analysis</a></dd>\n<dd><a href=\"/nh_snowcover/\">Satellite Products</a></dd>\n<dd><a href=\"/forecasts/\">Forecasts</a></dd>\n<dd><a href=\"/archived_data/\" title=\"Information on where to acquire NOHRSC raster data\">Data Archive</a></dd>\n<dd><a href=\"/shef_archive/\" title=\"Browse an archive of SHEF text messages\">SHEF Products</a></dd>\n</dl>\n<form name=\"nearest\" action=\"/nearest/index.html\">\n<dl>\n<dt>Observations near</dt>\n<dd><input type=\"text\" size=\"9\" name=\"city\" value=\"City, ST\" title=\"Search for snow observations near a city. Press enter or select the go button to submit request\" onfocus=\"this.value='';\">\n<input type=\"submit\" value=\"Go\"></dd>\n</dl>\n</form>\n<dl>\n<dt>Science/Technology</dt>\n<dd><a href=\"/technology/\" title=\"More detailed information about the NOHRSC\">NOHRSC</a></dd>\n<dd><a href=\"/gisdatasets/\" title=\"Shapefiles available for download\">GIS Data Sets</a></dd>\n<dd><a href=\"/special/\" title=\"Satellite/GIS images for certain projects\">Special Purpose Imagery</a></dd>\n</dl>\n<dl>\n<dt>About The NOHRSC</dt>\n<dd><a href=\"/directory/\" title=\"Meet the staff at the NOHRSC\">Staff</a></dd>\n</dl>\n<dl>\n<dt>NOAA Links</dt>\n<dd><a href=\"https://www.ncdc.noaa.gov/snow-and-ice/\">Snow Climatology</a></dd>\n<dd><a href=\"/links.html\">Related Links</a></dd>\n</dl>\n<dl>\n<dt>Help</dt>\n<dd><a href=\"/help/\" title=\"NOHRSC web site help\">Help and FAQ</a></dd>\n<dd><a href=\"/sitemap.html\">Site Map</a></dd>\n</dl>\n<dl>\n<dt>Contact Us</dt>\n<dd><a href=\"/contact.html\">Please Send Us Comments!</a></dd>\n</dl>\n<center>\n<a href=\"https://www.usa.gov\"><img src=\"/images/usagov_logo_color_110w.gif\" alt=\"USA.gov is the U.S. Government's official Web portal to all Federal, state and local government Web resources and services.\" width=\"110\" height=\"30\" border=\"0\"></a>\n<br>\n</center>\n\t\t\t</td>\n\t\t\t<td id=\"content_block\">\n\t\t\t\t<a href=\"\" name=\"content\"></a>\n<center><strong>Nearest observations to</strong></center>\n<center><h2>40.05&deg;N, -106.04&deg;W</h2></center>\n<strong>Note: these data are unofficial and provisional.</strong><br>\n<form name=\"data\" action=\"/nearest/index.html\">\n<fieldset>\n<legend><b>Location and Date</b></legend>\n<label title=\"Enter City, ST (or Latitude, Longitude)\">Enter your \"City, ST (or Latitude, Longitude)\" <input type=\"text\" size=\"16\" maxlength=\"80\" name=\"city\" value=\"40.05&deg;N, -106.04&deg;W\"></label>\n<input type=\"hidden\" name=\"county\" value=\"\">\n<input type=\"submit\" value=\"Go\" title=\"Click to refresh screen\"><br><br>\n<input type=\"hidden\" name=\"l\" value=\"5\">\n<select name=\"u\" title=\"Units\" size=\"1\" class=\"smallform\">\n<option value=\"e\" class=\"smallform\" selected>English</option>\n<option value=\"m\" class=\"smallform\">Metric</option>\n</select>\n &nbsp;\n<select name=\"y\" title=\"Year\" size=\"1\" class=\"smallform\">\n<option value=\"2003\" class=\"smallform\">2003</option>\n<option value=\"2004\" class=\"smallform\">2004</option>\n<option value=\"2005\" class=\"smallform\">2005</option>\n<option value=\"2006\" class=\"smallform\">2006</option>\n<option value=\"2007\" class=\"smallform\">2007</option>\n<option value=\"2008\" class=\"smallform\">2008</option>\n<option value=\"2009\" class=\"smallform\">2009</option>\n<option value=\"2010\" class=\"smallform\">2010</option>\n<option value=\"2011\" class=\"smallform\">2011</option>\n<option value=\"2012\" class=\"smallform\">2012</option>\n<option value=\"2013\" class=\"smallform\">2013</option>\n<option value=\"2014\" class=\"smallform\">2014</option>\n<option value=\"2015\" class=\"smallform\">2015</option>\n<option value=\"2016\" class=\"smallform\">2016</option>\n<option value=\"2017\" class=\"smallform\">2017</option>\n<option value=\"2018\" class=\"smallform\">2018</option>\n<option value=\"2019\" class=\"smallform\">2019</option>\n<option value=\"2020\" class=\"smallform\">2020</option>\n<option value=\"2021\" class=\"smallform\">2021</option>\n<option value=\"2022\" class=\"smallform\" selected>2022</option>\n</select>\n &nbsp;\n<select name=\"m\" title=\"Month\" size=\"1\" class=\"smallform\">\n<option value=\"1\" class=\"smallform\">January</option>\n<option value=\"2\" class=\"smallform\">February</option>\n<option value=\"3\" class=\"smallform\">March</option>\n<option value=\"4\" class=\"smallform\">April</option>\n<option value=\"5\" class=\"smallform\" selected>May</option>\n<option value=\"6\" class=\"smallform\">June</option>\n<option value=\"7\" class=\"smallform\">July</option>\n<option value=\"8\" class=\"smallform\">August</option>\n<option value=\"9\" class=\"smallform\">September</option>\n<option value=\"10\" class=\"smallform\">October</option>\n<option value=\"11\" class=\"smallform\">November</option>\n<option value=\"12\" class=\"smallform\">December</option>\n</select>\n &nbsp;\n<select name=\"d\" title=\"Day\" size=\"1\" class=\"smallform\">\n<option value=\"1\" class=\"smallform\">1</option>\n<option value=\"2\" class=\"smallform\">2</option>\n<option value=\"3\" class=\"smallform\">3</option>\n<option value=\"4\" class=\"smallform\" selected>4</option>\n<option value=\"5\" class=\"smallform\">5</option>\n<option value=\"6\" class=\"smallform\">6</option>\n<option value=\"7\" class=\"smallform\">7</option>\n<option value=\"8\" class=\"smallform\">8</option>\n<option value=\"9\" class=\"smallform\">9</option>\n<option value=\"10\" class=\"smallform\">10</option>\n<option value=\"11\" class=\"smallform\">11</option>\n<option value=\"12\" class=\"smallform\">12</option>\n<option value=\"13\" class=\"smallform\">13</option>\n<option value=\"14\" class=\"smallform\">14</option>\n<option value=\"15\" class=\"smallform\">15</option>\n<option value=\"16\" class=\"smallform\">16</option>\n<option value=\"17\" class=\"smallform\">17</option>\n<option value=\"18\" class=\"smallform\">18</option>\n<option value=\"19\" class=\"smallform\">19</option>\n<option value=\"20\" class=\"smallform\">20</option>\n<option value=\"21\" class=\"smallform\">21</option>\n<option value=\"22\" class=\"smallform\">22</option>\n<option value=\"23\" class=\"smallform\">23</option>\n<option value=\"24\" class=\"smallform\">24</option>\n<option value=\"25\" class=\"smallform\">25</option>\n<option value=\"26\" class=\"smallform\">26</option>\n<option value=\"27\" class=\"smallform\">27</option>\n<option value=\"28\" class=\"smallform\">28</option>\n<option value=\"29\" class=\"smallform\">29</option>\n<option value=\"30\" class=\"smallform\">30</option>\n<option value=\"31\" class=\"smallform\">31</option>\n</select>\n &nbsp;\n<input type=\"submit\" name=\"i\" value=\" - \" title=\"Back one day\"> &nbsp;\n<input type=\"submit\" name=\"i\" value=\" + \" title=\"Forward one day\"> &nbsp;\n</fieldset>\n</form>\n<table width=\"100%\"><tr><td><strong>Closest 5  observations near 40.05&deg;N, -106.04&deg;W</strong><br>40.05&deg;N, -106.04&deg;W (Elevation: N/A)</td>\n<td align=\"right\">Latest between <span class=\"date\">2022-05-04 06:00 UTC</span><br>and  <span class=\"date\">2022-05-05 06:00 UTC</span></td></tr></table><hr>\n<table class=\"gray_data_table\" cellspacing=\"2\" summary=\"Table of Raw Snowfall Observations near 40.05&deg;N, -106.04&deg;W, \" width=\"100%\">\n<caption><strong>Raw Snowfall Observations</strong></caption>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <thead>\n  <tr>\n<th scope=\"col\">Station ID</th><th scope=\"col\">Name</th><th scope=\"col\">Elev.<br> (ft)</th><th scope=\"col\">Raw Snowfall<br> (in)</th><th scope=\"col\">Duration<br> (hours)</th><th scope=\"col\">Date (UTC)</th><th scope=\"col\">Distance</th></tr>\n </thead>\n <tbody>\n<tr><td class=\"desc\" title=\"(40.0071&deg;N, -105.8862&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-68\">CO-GR-68</a></td><td class=\"desc\">TABERNASH 2.7 NW, CO</td><td>8806</td><td>3.70</td><td>24</td><td>2022-05-04 13</td><td>8.7 mi ESE</td></tr>\n<tr><td class=\"desc\" title=\"(40.0375&deg;N, -106.203&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=WIFC2\">WIFC2</a></td><td class=\"desc\">WILLIAMS FORK DAM</td><td>7733</td><td>2.00</td><td>24</td><td>2022-05-04 14</td><td>8.7 mi W</td></tr>\n<tr><td class=\"desc\" title=\"(40.0911&deg;N, -106.2&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-52\">CO-GR-52</a></td><td class=\"desc\">PARSHALL 3.0 NNW, CO</td><td>7904</td><td>2.20</td><td>24</td><td>2022-05-04 13</td><td>8.9 mi WNW</td></tr>\n<tr><td class=\"desc\" title=\"(40.0015&deg;N, -105.8725&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-53\">CO-GR-53</a></td><td class=\"desc\">TABERNASH 1.9 NW, CO</td><td>8579</td><td>6.50</td><td>24</td><td>2022-05-04 13</td><td>9.5 mi ESE</td></tr>\n<tr><td class=\"desc\" title=\"(40.1997&deg;N, -105.9258&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-73\">CO-GR-73</a></td><td class=\"desc\">GRANBY 7.7 N, CO</td><td>8563</td><td>4.00</td><td>24</td><td>2022-05-04 13</td><td>12 mi NE</td></tr>\n</tbody></table>\n<hr>\n<table class=\"gray_data_table\" cellspacing=\"2\" summary=\"Table of Snow Depth Observations near 40.05&deg;N, -106.04&deg;W, \" width=\"100%\">\n<caption><strong>Snow Depth Observations</strong></caption>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <thead>\n  <tr>\n<th scope=\"col\">Station ID</th><th scope=\"col\">Name</th><th scope=\"col\">Elev.<br> (ft)</th><th scope=\"col\">Snow Depth<br> (in)</th><th scope=\"col\">Date (UTC)</th><th scope=\"col\">Distance</th></tr>\n </thead>\n <tbody>\n<tr><td class=\"desc\" title=\"(40.0071&deg;N, -105.8862&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-68\">CO-GR-68</a></td><td class=\"desc\">TABERNASH 2.7 NW, CO</td><td>8806</td><td>4.00</td><td>2022-05-04 13</td><td>8.7 mi ESE</td></tr>\n<tr><td class=\"desc\" title=\"(40.0375&deg;N, -106.203&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=WIFC2\">WIFC2</a></td><td class=\"desc\">WILLIAMS FORK DAM</td><td>7733</td><td>1.00</td><td>2022-05-04 14</td><td>8.7 mi W</td></tr>\n<tr><td class=\"desc\" title=\"(40.0911&deg;N, -106.2&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-52\">CO-GR-52</a></td><td class=\"desc\">PARSHALL 3.0 NNW, CO</td><td>7904</td><td>1.50</td><td>2022-05-04 13</td><td>8.9 mi WNW</td></tr>\n<tr><td class=\"desc\" title=\"(40.2254&deg;N, -105.9198&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=SCSC2\">SCSC2</a></td><td class=\"desc\">STILLWATER CREEK</td><td>8793</td><td>0.00</td><td>2022-05-05 05</td><td>13.7 mi NE</td></tr>\n<tr><td class=\"desc\" title=\"(40.2082&deg;N, -105.8634&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-78\">CO-GR-78</a></td><td class=\"desc\">GRAND LAKE 3.7 SW, CO</td><td>8537</td><td>3.00</td><td>2022-05-04 13</td><td>14.4 mi NE</td></tr>\n</tbody></table>\n<hr>\n<table class=\"gray_data_table\" cellspacing=\"2\" summary=\"Table of Snow Water Equivalent Observations near 40.05&deg;N, -106.04&deg;W, \" width=\"100%\">\n<caption><strong>Snow Water Equivalent Observations</strong></caption>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <thead>\n  <tr>\n<th scope=\"col\">Station ID</th><th scope=\"col\">Name</th><th scope=\"col\">Elev.<br> (ft)</th><th scope=\"col\">Snow Water Equivalent<br> (in)</th><th scope=\"col\">Date (UTC)</th><th scope=\"col\">Distance</th></tr>\n </thead>\n <tbody>\n<tr><td class=\"desc\" title=\"(40.2254&deg;N, -105.9198&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=SCSC2\">SCSC2</a></td><td class=\"desc\">STILLWATER CREEK</td><td>8793</td><td>0.00</td><td>2022-05-05 05</td><td>13.7 mi NE</td></tr>\n<tr><td class=\"desc\" title=\"(39.8687&deg;N, -105.8675&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=FCVC2\">FCVC2</a></td><td class=\"desc\">FOOL CREEK</td><td>11168</td><td>20.30</td><td>2022-05-04 07</td><td>15.5 mi SE</td></tr>\n<tr><td class=\"desc\" title=\"(39.7956&deg;N, -106.0273&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=MFKC2\">MFKC2</a></td><td class=\"desc\">MIDDLE FORK CAMP</td><td>8983</td><td>1.40</td><td>2022-05-05 05</td><td>17.6 mi S</td></tr>\n<tr><td class=\"desc\" title=\"(40.347&deg;N, -106.0943&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=WLLC2\">WLLC2</a></td><td class=\"desc\">WILLOW CREEK PASS</td><td>9600</td><td>14.70</td><td>2022-05-05 05</td><td>20.7 mi N</td></tr>\n<tr><td class=\"desc\" title=\"(39.7645&deg;N, -105.9062&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=JNPC2\">JNPC2</a></td><td class=\"desc\">JONES PASS</td><td>10482</td><td>10.80</td><td>2022-05-05 05</td><td>20.9 mi SSE</td></tr>\n</tbody></table>\n<hr>\n<table class=\"gray_data_table\" cellspacing=\"2\" summary=\"Table of Raw Precipitation Observations near 40.05&deg;N, -106.04&deg;W, \" width=\"100%\">\n<caption><strong>Raw Precipitation Observations</strong></caption>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <colgroup>\n <thead>\n  <tr>\n<th scope=\"col\">Station ID</th><th scope=\"col\">Name</th><th scope=\"col\">Elev.<br> (ft)</th><th scope=\"col\">Raw Precipitation<br> (in)</th><th scope=\"col\">Duration<br> (hours)</th><th scope=\"col\">Date (UTC)</th><th scope=\"col\">Distance</th></tr>\n </thead>\n <tbody>\n<tr><td class=\"desc\" title=\"(40.1083&deg;N, -106.0036&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CAWC2\">CAWC2</a></td><td class=\"desc\">COLORADO RVR BLW WINDY GAP</td><td>7822</td><td>0.23</td><td>24</td><td>2022-05-05 06</td><td>4.5 mi NNE</td></tr>\n<tr><td class=\"desc\" title=\"(40.1186&deg;N, -105.8997&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=CO-GR-81\">CO-GR-81</a></td><td class=\"desc\">GRANBY 2.9 NE, CO</td><td>8041</td><td>0.65</td><td>24</td><td>2022-05-05 01</td><td>8.8 mi ENE</td></tr>\n<tr><td class=\"desc\" title=\"(40.136&deg;N, -106.1744&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=GRUC2\">GRUC2</a></td><td class=\"desc\">GROUSE MOUNTAIN</td><td>10013</td><td>0.00</td><td>1</td><td>2022-05-05 05</td><td>9.3 mi WNW</td></tr>\n<tr><td class=\"desc\" title=\"(39.8906&deg;N, -106.0367&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=KSEC2\">KSEC2</a></td><td class=\"desc\">KEYSER RIDGE</td><td>10190</td><td>0.00</td><td>1</td><td>2022-05-05 05</td><td>11 mi S</td></tr>\n<tr><td class=\"desc\" title=\"(39.8906&deg;N, -106.0367&deg;W)\"><a href=\"/interactive/html/graph.html?units=0&amp;ey=2022&amp;em=5&amp;ed=7&amp;eh=6&amp;station=KSEC2\">KSEC2</a></td><td class=\"desc\">KEYSER RIDGE</td><td>10190</td><td>0.29</td><td>24</td><td>2022-05-05 05</td><td>11 mi S</td></tr>\n</tbody></table>\n<br>\n<span style=\"color: white\">Page generated in 2.12697 seconds.</span><br>\n\t\t\t\t<br><br>\n\t\t\t\t<table align=\"center\" cellspacing=\"2\" cellpadding=\"2\" border=\"0\">\n\t\t\t\t\t<tr align=\"center\">\n\t\t\t\t\t\t<td>\n\t\t\t\t\t\t\tNOHRSC<br>\n\t\t\t\t\t\t\t<a href=\"/mission.html\">Mission Statement</a>\n\t\t\t\t\t\t\t&nbsp;|&nbsp;\n\t\t\t\t\t\t\t<a href=\"/contact.html\">Contact</a>\n\t\t\t\t\t\t</td>\n\t\t\t\t\t</tr>\n\t\t\t\t</table>\n\t\t\t\t<table width=\"100%\" cellspacing=\"0\" cellpadding=\"0\" border=\"0\">\n\t\t\t\t\t<tr>\n\t\t\t\t\t\t<td colspan=\"3\"><hr></td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr valign=\"top\"> \n\t\t\t\t\t\t<td align=\"left\" class=\"gray\">\n\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov\"><span class=\"gray\">National Weather Service</span></a><br>\n\t\t\t\t\t\t\tNational Operational Hydrologic Remote Sensing Center<br>\n                                                        <a href=\"https://water.noaa.gov\"><span class=\"gray\">Office of Water Prediction</span></a><br>\n\t\t\t\t\t\t\t1735 Lake Drive W.<br>\n\t\t\t\t\t\t\tChanhassen, MN 55317<br>\n\t\t\t\t\t\t\t<br>\n\t\t\t\t\t\t</td>\n\t\t\t\t\t\t<td align=\"right\">\n\t\t\t\t\t\t\t<a href=\"/\"><img src=\"/images/nohrsc.png\" alt=\"NOHRSC homepage\" border=0></a>\n\t\t\t\t\t\t</td>\n\t\t\t\t\t</tr>\n\t\t\t\t\t<tr valign=\"top\">\n\t\t\t\t\t\t<td align=\"left\" class=\"gray\">\n\t\t\t\t\t\t\t<a href=\"/contact.html\"><span class=\"gray\">Contact NOHRSC</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/glossary/\"><span class=\"gray\">Glossary</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/credits.php\"><span class=\"gray\">Credits</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.cio.noaa.gov/Policy_Programs/info_quality.html\"><span class=\"gray\">Information Quality</span></a><br>\nPage last modified: Nov 20, 2019 <br>\n\t\t\t\t\t\t</td>\n\t\t\t\t\t\t<td align=\"right\" class=\"gray\">\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/admin.php\"><span class=\"gray\">About Us</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/disclaimer.php\"><span class=\"gray\">Disclaimer</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/privacy.php\"><span class=\"gray\">Privacy Policy</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.rdc.noaa.gov/~foia/\"><span class=\"gray\">FOIA</span></a><br>\n\t\t\t\t\t\t\t<a href=\"https://www.weather.gov/careers.php\"><span class=\"gray\">Career Opportunities</span></a><br>\n\t\t\t\t\t\t</td>\n\t\t\t\t\t</tr>\n\t\t\t\t</table>\n\t\t\t</td>\n\t\t</tr>\n\t</table>\n</body>\n</html>\n/Users/joe/gw-workspace/t5ck5kqz27u/data_snotel_real_time.py:28: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\nThe code that caused this warning is on line 28 of the file /Users/joe/gw-workspace/t5ck5kqz27u/data_snotel_real_time.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n  parsed_html = BeautifulSoup(webContent)\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/t5ck5kqz27u/data_snotel_real_time.py\", line 29, in <module>\n    print(parsed_html.body.find('div', attrs={'class':'container'}).text)\nAttributeError: 'NoneType' object has no attribute 'text'\n",
  "history_begin_time" : 1667410545202,
  "history_end_time" : 1667410624030,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qh9i9u9nhuj",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\numpy\\__init__.py:148: UserWarning: mkl-service package failed to import, therefore Intel(R) MKL initialization ensuring its correct out-of-the box operation under condition when Gnu OpenMP had already been loaded by Python process is not assured. Please install mkl-service package, see http://github.com/IntelPython/mkl-service\n  from . import _distributor_init\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\qh9i9u9nhuj\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\metloom\\pointdata\\__init__.py\", line 1, in <module>\n    from .base import PointData, PointDataCollection\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\metloom\\pointdata\\base.py\", line 5, in <module>\n    import pandas as pd\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\site-packages\\pandas\\__init__.py\", line 16, in <module>\n    raise ImportError(\nImportError: Unable to import required dependencies:\nnumpy: \nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\nWe have compiled some common reasons and troubleshooting tips at:\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\nPlease note and check the following:\n  * The Python version is: Python3.9 from \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\python.exe\"\n  * The NumPy version is: \"1.21.5\"\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\nOriginal error was: DLL load failed while importing _multiarray_umath: The specified module could not be found.\n",
  "history_begin_time" : 1667410386197,
  "history_end_time" : 1667410556070,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "g3inocjtwsa",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1667409181957,
  "history_end_time" : 1667409181974,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ZDPiMEr28hff",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n    for j in range(1):\n      latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n      longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n      response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n      webContent = response.read().decode('UTF-8')\n      parsed_html = BeautifulSoup(webContent)\n\n      snow_water_eq_table = parsed_html.find_all('table')[7]\n      nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n      stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n    if stations[i] != '\\nNo observations within 62 miles\\n':\n\n      for j in range(end_date.year - start_date.year + 1):\n\n        response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n        webContent = response.read().decode('UTF-8')\n        parsed_html = BeautifulSoup(webContent)\n\n        data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n        for k in range(len(data_table)):\n          new_cell_IDs.append(cell_ID[i])\n          dates.append(data_table[k].find_all('td')[0].text)\n          snow_water_eq.append(data_table[k].find_all('td')[2].text)\n    else:\n      new_cell_IDs.append(cell_ID[i])\n      dates.append('N/A')\n      snow_water_eq.append('N/A')\n\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "Running",
  "history_begin_time" : 1657723764825,
  "history_end_time" : null,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Running"
},{
  "history_id" : "SfJGJqI8KnVZ",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n    for j in range(1):\n      latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n      longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n      response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n      webContent = response.read().decode('UTF-8')\n      parsed_html = BeautifulSoup(webContent)\n\n      snow_water_eq_table = parsed_html.find_all('table')[7]\n      nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n      stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\n# def get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n#   print(f\"Retrieving data from {start_date} to {end_date}\")\n#   # read the grid geometry file\n#   homedir = os.path.expanduser('~')\n#   print(homedir)\n#   # read grid cell\n#   github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n#   # read grid cell\n#   #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n#   #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n#   #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#   stations = get_nearest_stations_for_all_grids()\n\n#   start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n#   end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n#   # NOTE: this url only allows user to query one year's worth of data\n#   nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n#   dates = []\n#   snow_water_eq = []\n#   # Keep track of cell IDs for each time series\n#   new_cell_IDs = []\n\n#   # Retrieve time series snow water equivalent data for each grid cell's nearest station\n#   for i in range(len(stations)):\n\n#     if stations[i] != '\\nNo observations within 62 miles\\n':\n\n#       for j in range(end_date.year - start_date.year + 1):\n\n#         response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n#         webContent = response.read().decode('UTF-8')\n#         parsed_html = BeautifulSoup(webContent)\n\n#         data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n#         for k in range(len(data_table)):\n#           new_cell_IDs.append(cell_ID[i])\n#           dates.append(data_table[k].find_all('td')[0].text)\n#           snow_water_eq.append(data_table[k].find_all('td')[2].text)\n#     else:\n#       new_cell_IDs.append(cell_ID[i])\n#       dates.append('N/A')\n#       snow_water_eq.append('N/A')\n\n#   df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n#   df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n#   print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "Running",
  "history_begin_time" : 1657723643446,
  "history_end_time" : 1657723761625,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Ll9HqIE6XbfA",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n#   # Retrieve nearest stations for each grid cell\n#   for i in range(int(len(cell_ID))):\n#     for j in range(1):\n#       latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n#       longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n#       response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n#       webContent = response.read().decode('UTF-8')\n#       parsed_html = BeautifulSoup(webContent)\n\n#       snow_water_eq_table = parsed_html.find_all('table')[7]\n#       nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n#       stations.append(nearest_station)\n  \n#   # save stations somewhere for cache\n  \n#   print(\"Nearest station list is completed. Saving to cache..\")\n  \n#   with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n#     pickle.dump(stations, fp)\n          \n#   return stations\n\n# def get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n#   print(f\"Retrieving data from {start_date} to {end_date}\")\n#   # read the grid geometry file\n#   homedir = os.path.expanduser('~')\n#   print(homedir)\n#   # read grid cell\n#   github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n#   # read grid cell\n#   #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n#   #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n#   #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#   stations = get_nearest_stations_for_all_grids()\n\n#   start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n#   end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n#   # NOTE: this url only allows user to query one year's worth of data\n#   nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n#   dates = []\n#   snow_water_eq = []\n#   # Keep track of cell IDs for each time series\n#   new_cell_IDs = []\n\n#   # Retrieve time series snow water equivalent data for each grid cell's nearest station\n#   for i in range(len(stations)):\n\n#     if stations[i] != '\\nNo observations within 62 miles\\n':\n\n#       for j in range(end_date.year - start_date.year + 1):\n\n#         response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n#         webContent = response.read().decode('UTF-8')\n#         parsed_html = BeautifulSoup(webContent)\n\n#         data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n#         for k in range(len(data_table)):\n#           new_cell_IDs.append(cell_ID[i])\n#           dates.append(data_table[k].find_all('td')[0].text)\n#           snow_water_eq.append(data_table[k].find_all('td')[2].text)\n#     else:\n#       new_cell_IDs.append(cell_ID[i])\n#       dates.append('N/A')\n#       snow_water_eq.append('N/A')\n\n#   df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n#   df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n#   print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/Ll9HqIE6XbfA', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\ntesting...\nStart to get nearest stations..\n",
  "history_begin_time" : 1657723632608,
  "history_end_time" : 1657723640864,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2K5NVRTCsW5c",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n#   nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n#   query_urls = []\n  \n#   stations = []\n  \n#   current_date = datetime.now()\n\n#   # Retrieve nearest stations for each grid cell\n#   for i in range(int(len(cell_ID))):\n#     for j in range(1):\n#       latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n#       longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n#       response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n#       webContent = response.read().decode('UTF-8')\n#       parsed_html = BeautifulSoup(webContent)\n\n#       snow_water_eq_table = parsed_html.find_all('table')[7]\n#       nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n#       stations.append(nearest_station)\n  \n#   # save stations somewhere for cache\n  \n#   print(\"Nearest station list is completed. Saving to cache..\")\n  \n#   with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n#     pickle.dump(stations, fp)\n          \n#   return stations\n\n# def get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n#   print(f\"Retrieving data from {start_date} to {end_date}\")\n#   # read the grid geometry file\n#   homedir = os.path.expanduser('~')\n#   print(homedir)\n#   # read grid cell\n#   github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n#   # read grid cell\n#   #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n#   #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n#   #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#   stations = get_nearest_stations_for_all_grids()\n\n#   start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n#   end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n#   # NOTE: this url only allows user to query one year's worth of data\n#   nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n#   dates = []\n#   snow_water_eq = []\n#   # Keep track of cell IDs for each time series\n#   new_cell_IDs = []\n\n#   # Retrieve time series snow water equivalent data for each grid cell's nearest station\n#   for i in range(len(stations)):\n\n#     if stations[i] != '\\nNo observations within 62 miles\\n':\n\n#       for j in range(end_date.year - start_date.year + 1):\n\n#         response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n#         webContent = response.read().decode('UTF-8')\n#         parsed_html = BeautifulSoup(webContent)\n\n#         data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n#         for k in range(len(data_table)):\n#           new_cell_IDs.append(cell_ID[i])\n#           dates.append(data_table[k].find_all('td')[0].text)\n#           snow_water_eq.append(data_table[k].find_all('td')[2].text)\n#     else:\n#       new_cell_IDs.append(cell_ID[i])\n#       dates.append('N/A')\n#       snow_water_eq.append('N/A')\n\n#   df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n#   df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n#   print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/2K5NVRTCsW5c', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\ntesting...\nStart to get nearest stations..\n",
  "history_begin_time" : 1657723618325,
  "history_end_time" : 1657723626628,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dbKrjd9FlSS7",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n#   # Retrieve and store cell IDs and their corresponding coordinates\n#   cell_ID = []\n#   ID_to_coord = {}\n\n#   for i in range(len(grid_cells['features'])):\n#     id = grid_cells['features'][i]['properties']['cell_id']\n#     coords = grid_cells['features'][i]['geometry']['coordinates']\n#     if id is not None and coords is not None: \n#       cell_ID.append(id)\n#       ID_to_coord[id] = coords\n\n#   nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n#   query_urls = []\n  \n#   stations = []\n  \n#   current_date = datetime.now()\n\n#   # Retrieve nearest stations for each grid cell\n#   for i in range(int(len(cell_ID))):\n#     for j in range(1):\n#       latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n#       longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n#       response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n#       webContent = response.read().decode('UTF-8')\n#       parsed_html = BeautifulSoup(webContent)\n\n#       snow_water_eq_table = parsed_html.find_all('table')[7]\n#       nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n#       stations.append(nearest_station)\n  \n#   # save stations somewhere for cache\n  \n#   print(\"Nearest station list is completed. Saving to cache..\")\n  \n#   with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n#     pickle.dump(stations, fp)\n          \n#   return stations\n\n# def get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n#   print(f\"Retrieving data from {start_date} to {end_date}\")\n#   # read the grid geometry file\n#   homedir = os.path.expanduser('~')\n#   print(homedir)\n#   # read grid cell\n#   github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n#   # read grid cell\n#   #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n#   #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n#   #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#   stations = get_nearest_stations_for_all_grids()\n\n#   start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n#   end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n#   # NOTE: this url only allows user to query one year's worth of data\n#   nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n#   dates = []\n#   snow_water_eq = []\n#   # Keep track of cell IDs for each time series\n#   new_cell_IDs = []\n\n#   # Retrieve time series snow water equivalent data for each grid cell's nearest station\n#   for i in range(len(stations)):\n\n#     if stations[i] != '\\nNo observations within 62 miles\\n':\n\n#       for j in range(end_date.year - start_date.year + 1):\n\n#         response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n#         webContent = response.read().decode('UTF-8')\n#         parsed_html = BeautifulSoup(webContent)\n\n#         data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n#         for k in range(len(data_table)):\n#           new_cell_IDs.append(cell_ID[i])\n#           dates.append(data_table[k].find_all('td')[0].text)\n#           snow_water_eq.append(data_table[k].find_all('td')[2].text)\n#     else:\n#       new_cell_IDs.append(cell_ID[i])\n#       dates.append('N/A')\n#       snow_water_eq.append('N/A')\n\n#   df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n#   df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n#   print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/dbKrjd9FlSS7', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\ntesting...\nStart to get nearest stations..\n",
  "history_begin_time" : 1657723604044,
  "history_end_time" : 1657723612456,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "C0d9Lsg5NR1Y",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n#   filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n#   # Read in grid cells\n#   with open(filepath) as f:\n#     grid_cells = geojson.load(f)\n  \n#   # Retrieve and store cell IDs and their corresponding coordinates\n#   cell_ID = []\n#   ID_to_coord = {}\n\n#   for i in range(len(grid_cells['features'])):\n#     id = grid_cells['features'][i]['properties']['cell_id']\n#     coords = grid_cells['features'][i]['geometry']['coordinates']\n#     if id is not None and coords is not None: \n#       cell_ID.append(id)\n#       ID_to_coord[id] = coords\n\n#   nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n#   query_urls = []\n  \n#   stations = []\n  \n#   current_date = datetime.now()\n\n#   # Retrieve nearest stations for each grid cell\n#   for i in range(int(len(cell_ID))):\n#     for j in range(1):\n#       latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n#       longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n#       response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n#       webContent = response.read().decode('UTF-8')\n#       parsed_html = BeautifulSoup(webContent)\n\n#       snow_water_eq_table = parsed_html.find_all('table')[7]\n#       nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n#       stations.append(nearest_station)\n  \n#   # save stations somewhere for cache\n  \n#   print(\"Nearest station list is completed. Saving to cache..\")\n  \n#   with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n#     pickle.dump(stations, fp)\n          \n#   return stations\n\n# def get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n#   print(f\"Retrieving data from {start_date} to {end_date}\")\n#   # read the grid geometry file\n#   homedir = os.path.expanduser('~')\n#   print(homedir)\n#   # read grid cell\n#   github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n#   # read grid cell\n#   #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n#   #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n#   #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#   stations = get_nearest_stations_for_all_grids()\n\n#   start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n#   end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n#   # NOTE: this url only allows user to query one year's worth of data\n#   nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n#   dates = []\n#   snow_water_eq = []\n#   # Keep track of cell IDs for each time series\n#   new_cell_IDs = []\n\n#   # Retrieve time series snow water equivalent data for each grid cell's nearest station\n#   for i in range(len(stations)):\n\n#     if stations[i] != '\\nNo observations within 62 miles\\n':\n\n#       for j in range(end_date.year - start_date.year + 1):\n\n#         response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n#         webContent = response.read().decode('UTF-8')\n#         parsed_html = BeautifulSoup(webContent)\n\n#         data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n#         for k in range(len(data_table)):\n#           new_cell_IDs.append(cell_ID[i])\n#           dates.append(data_table[k].find_all('td')[0].text)\n#           snow_water_eq.append(data_table[k].find_all('td')[2].text)\n#     else:\n#       new_cell_IDs.append(cell_ID[i])\n#       dates.append('N/A')\n#       snow_water_eq.append('N/A')\n\n#   df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n#   df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n#   print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/C0d9Lsg5NR1Y', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\ntesting...\nStart to get nearest stations..\n",
  "history_begin_time" : 1657723582455,
  "history_end_time" : 1657723590128,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pausxvA6pPK7",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n    grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n    for j in range(1):\n      latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n      longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n      response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n      webContent = response.read().decode('UTF-8')\n      parsed_html = BeautifulSoup(webContent)\n\n      snow_water_eq_table = parsed_html.find_all('table')[7]\n      nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n      stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n    if stations[i] != '\\nNo observations within 62 miles\\n':\n\n      for j in range(end_date.year - start_date.year + 1):\n\n        response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n        webContent = response.read().decode('UTF-8')\n        parsed_html = BeautifulSoup(webContent)\n\n        data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n        for k in range(len(data_table)):\n          new_cell_IDs.append(cell_ID[i])\n          dates.append(data_table[k].find_all('td')[0].text)\n          snow_water_eq.append(data_table[k].find_all('td')[2].text)\n    else:\n      new_cell_IDs.append(cell_ID[i])\n      dates.append('N/A')\n      snow_water_eq.append('N/A')\n\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "/Users/joe/gw-workspace/pausxvA6pPK7/data_snotel_real_time.py:75: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\nThe code that caused this warning is on line 75 of the file /Users/joe/gw-workspace/pausxvA6pPK7/data_snotel_real_time.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n  parsed_html = BeautifulSoup(webContent)\n",
  "history_begin_time" : 1657723522823,
  "history_end_time" : 1657723559609,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ndEC5Lk60Xnd",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n    with open(cache_file, \"rb\") as fp:\n      return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n    id = grid_cells['features'][i]['properties']['cell_id']\n    coords = grid_cells['features'][i]['geometry']['coordinates']\n    if id is not None and coords is not None: \n      cell_ID.append(id)\n      ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n    for j in range(1):\n      latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n      longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n      response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n      webContent = response.read().decode('UTF-8')\n      parsed_html = BeautifulSoup(webContent)\n\n      snow_water_eq_table = parsed_html.find_all('table')[7]\n      nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n      stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n    if stations[i] != '\\nNo observations within 62 miles\\n':\n\n      for j in range(end_date.year - start_date.year + 1):\n\n        response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n        webContent = response.read().decode('UTF-8')\n        parsed_html = BeautifulSoup(webContent)\n\n        data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n        for k in range(len(data_table)):\n          new_cell_IDs.append(cell_ID[i])\n          dates.append(data_table[k].find_all('td')[0].text)\n          snow_water_eq.append(data_table[k].find_all('td')[2].text)\n    else:\n      new_cell_IDs.append(cell_ID[i])\n      dates.append('N/A')\n      snow_water_eq.append('N/A')\n\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "Running",
  "history_begin_time" : 1657723483547,
  "history_end_time" : 1657723520113,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "tBAal9o8alnZ",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\nprint(\"testing...\")\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n      with open(cache_file, \"rb\") as fp:\n          return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "Running",
  "history_begin_time" : 1657723247572,
  "history_end_time" : 1657723479709,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "89n4CSFpv48D",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n      with open(cache_file, \"rb\") as fp:\n          return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "Running",
  "history_begin_time" : 1657723181795,
  "history_end_time" : 1657723222839,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "TXUSvrxuTsss",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\ndef get_nearest_stations_for_all_grids():\n  \n  print(\"Start to get nearest stations..\")\n  \n  cache_file = f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\"\n  \n  if os.path.exists(cache_file):\n      with open(, \"rb\") as fp:\n          return pickle.load(fp)\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n  \n  # save stations somewhere for cache\n  \n  print(\"Nearest station list is completed. Saving to cache..\")\n  \n  with open(f\"{github_dir}/data/snowcast_provided/nearest_stations_grid_eval.pkl\", \"wb\") as fp:\n    pickle.dump(stations, fp)\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  print(f\"Retrieving data from {start_date} to {end_date}\")\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"All SNOTEL time series are saved.\")\n\nget_nearest_stations_for_all_grids()\n#get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "  File \"/Users/joe/gw-workspace/TXUSvrxuTsss/data_snotel_real_time.py\", line 39\n    with open(, \"rb\") as fp:\n              ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1657723142399,
  "history_end_time" : 1657723142485,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "CSt2h6DtFqI6",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\n\ntry:\n  from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n  from bs4 import BeautifulSoup\n\ndef get_nearest_stations_for_all_grids():\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n          \n  # save stations somewhere for cache\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "/Users/joe/gw-workspace/CSt2h6DtFqI6/data_snotel_real_time.py:68: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\nThe code that caused this warning is on line 68 of the file /Users/joe/gw-workspace/CSt2h6DtFqI6/data_snotel_real_time.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n  parsed_html = BeautifulSoup(webContent)\n",
  "history_begin_time" : 1657722616402,
  "history_end_time" : 1657722907783,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "OSWtH3AM9hv8",
  "history_input" : "\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nfrom datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nprint(sys.path)\n\ndef get_nearest_stations_for_all_grids():\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n          \n  # save stations somewhere for cache\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/OSWtH3AM9hv8', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\n/Users/joe\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/OSWtH3AM9hv8/data_snotel_real_time.py\", line 132, in <module>\n    get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n  File \"/Users/joe/gw-workspace/OSWtH3AM9hv8/data_snotel_real_time.py\", line 90, in get_snotel_time_series\n    stations = get_nearest_stations_for_all_grids()\n  File \"/Users/joe/gw-workspace/OSWtH3AM9hv8/data_snotel_real_time.py\", line 62, in get_nearest_stations_for_all_grids\n    parsed_html = BeautifulSoup(webContent)\nNameError: name 'BeautifulSoup' is not defined\n",
  "history_begin_time" : 1657722575997,
  "history_end_time" : 1657722585169,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "1lfi8hciLztH",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nprint(sys.path)\n\ndef get_nearest_stations_for_all_grids():\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n  \n  current_date = datetime.now()\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = current_date.year, month = current_date.month, day = current_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n          \n  # save stations somewhere for cache\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/1lfi8hciLztH', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\n/Users/joe\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/1lfi8hciLztH/data_snotel_real_time.py\", line 130, in <module>\n    get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n  File \"/Users/joe/gw-workspace/1lfi8hciLztH/data_snotel_real_time.py\", line 88, in get_snotel_time_series\n    stations = get_nearest_stations_for_all_grids()\n  File \"/Users/joe/gw-workspace/1lfi8hciLztH/data_snotel_real_time.py\", line 49, in get_nearest_stations_for_all_grids\n    current_date = datetime.now()\nAttributeError: module 'datetime' has no attribute 'now'\n",
  "history_begin_time" : 1657722524243,
  "history_end_time" : 1657722533065,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ZtXLVBWGDmwb",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import *\nimport traceback\n\nprint(sys.path)\n\ndef get_nearest_stations_for_all_grids():\n  \n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n  \n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  \n  query_urls = []\n  \n  stations = []\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = start_date.year, month = start_date.month, day = start_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n          \n  # save stations somewhere for cache\n          \n  return stations\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  stations = get_nearest_stations_for_all_grids()\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/ZtXLVBWGDmwb', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\n/Users/joe\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/ZtXLVBWGDmwb/data_snotel_real_time.py\", line 128, in <module>\n    get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n  File \"/Users/joe/gw-workspace/ZtXLVBWGDmwb/data_snotel_real_time.py\", line 86, in get_snotel_time_series\n    stations = get_nearest_stations_for_all_grids()\n  File \"/Users/joe/gw-workspace/ZtXLVBWGDmwb/data_snotel_real_time.py\", line 56, in get_nearest_stations_for_all_grids\n    response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = start_date.year, month = start_date.month, day = start_date.day))\nNameError: name 'start_date' is not defined\n",
  "history_begin_time" : 1657722402661,
  "history_end_time" : 1657722413189,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ehMWOkjwzGkE",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import date\nfrom snowcast_utils import *\nimport traceback\n\nprint(sys.path)\n\ndef get_snotel_time_series(start_date = '2016-1-1', end_date = '2022-7-12'):\n  \n  \n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n\n\n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  test_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=1, day=1)\n  #print(test_noaa_query_url)\n\n  query_urls = []\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n  stations = []\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = start_date.year, month = start_date.month, day = start_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n\n\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n",
  "history_output" : "today date = 2022-07-13\n/Users/joe\n2022-06-25 06:00:00\n['/Users/joe/gw-workspace/ehMWOkjwzGkE', '/Users/l21-n02609-comm/opt/anaconda3/lib/python39.zip', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/lib-dynload', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/aeosa', '/Users/joe/Documents/GitHub/twitterscraper', '/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/IPython/extensions']\n/Users/joe\nTraceback (most recent call last):\n  File \"/Users/joe/gw-workspace/ehMWOkjwzGkE/data_snotel_real_time.py\", line 128, in <module>\n    get_snotel_time_series(\"2022-06-01\", \"2022-06-02\")\n  File \"/Users/joe/gw-workspace/ehMWOkjwzGkE/data_snotel_real_time.py\", line 68, in get_snotel_time_series\n    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\nAttributeError: module 'datetime' has no attribute 'strptime'\n",
  "history_begin_time" : 1657722177275,
  "history_end_time" : 1657722188251,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "mZgDbtZX493O",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import date\nfrom snowcast_utils import *\nimport traceback\n\nprint(sys.path)\n\ndef get_snotel_time_series():\n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n\n\n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  test_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=1, day=1)\n  #print(test_noaa_query_url)\n\n  start_date = '2016-1-1'\n  end_date = '2022-7-12'\n  query_urls = []\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n  stations = []\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = start_date.year, month = start_date.month, day = start_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n\n\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv\")\n  print(\"Saved\")\n\nget_snotel_time_series()\n",
  "history_output" : "",
  "history_begin_time" : 1657722125439,
  "history_end_time" : 1657722133477,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "i6L0QnAP3ReA",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# Write first python in Geoweaver\nimport pandas as pd\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nimport geojson\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import date\nfrom snowcast_utils import *\nimport traceback\n\nprint(sys.path)\n\ndef get_snotel_time_series():\n  \n  try:\n      from BeautifulSoup import BeautifulSoup\n  except ImportError:\n      from bs4 import BeautifulSoup\n\n  # read the grid geometry file\n  homedir = os.path.expanduser('~')\n  print(homedir)\n  # read grid cell\n  github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n  # read grid cell\n  #submission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n  #all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  #all_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n  filepath = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  # Read in grid cells\n  with open(filepath) as f:\n      grid_cells = geojson.load(f)\n\n\n  # Retrieve and store cell IDs and their corresponding coordinates\n  cell_ID = []\n  ID_to_coord = {}\n\n  for i in range(len(grid_cells['features'])):\n      id = grid_cells['features'][i]['properties']['cell_id']\n      coords = grid_cells['features'][i]['geometry']['coordinates']\n      if id is not None and coords is not None: \n          cell_ID.append(id)\n          ID_to_coord[id] = coords\n\n  nohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n  test_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=1, day=1)\n  #print(test_noaa_query_url)\n\n  start_date = '2016-1-1'\n  end_date = '2022-7-12'\n  query_urls = []\n\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n  stations = []\n\n  # Retrieve nearest stations for each grid cell\n  for i in range(int(len(cell_ID))):\n      for j in range(1):\n          latitude = round(ID_to_coord.get(cell_ID[i])[0][j][1], 2)\n          longitude = round(ID_to_coord.get(cell_ID[i])[0][j][0], 2)\n\n          #response = urllib.request.urlopen(test_noaa_query_url)\n          response = urllib.request.urlopen(nohrsc_url_format_string.format(lat = latitude, lon = longitude, year = start_date.year, month = start_date.month, day = start_date.day))\n          webContent = response.read().decode('UTF-8')\n          parsed_html = BeautifulSoup(webContent)\n\n          snow_water_eq_table = parsed_html.find_all('table')[7]\n          nearest_station = snow_water_eq_table.find_all('td')[0].text\n\n          stations.append(nearest_station)\n\n\n  # NOTE: this url only allows user to query one year's worth of data\n  nohrsc_time_series_url = 'https://www.nohrsc.noaa.gov/interactive/html/graph.html?station={station}&w=600&h=400&o=a&uc=0&by={start_year}&bm={start_month}&bd={start_day}&bh=0&ey={end_year}&em={end_month}&ed={end_day}&eh=23&data=1&units=0&region=us'\n\n  dates = []\n  snow_water_eq = []\n  # Keep track of cell IDs for each time series\n  new_cell_IDs = []\n\n  # Retrieve time series snow water equivalent data for each grid cell's nearest station\n  for i in range(len(stations)):\n\n      if stations[i] != '\\nNo observations within 62 miles\\n':\n\n          for j in range(end_date.year - start_date.year + 1):\n\n              response = urllib.request.urlopen(nohrsc_time_series_url.format(station = stations[i], start_year = start_date.year + j, start_month = start_date.month, start_day = start_date.day, end_year = end_date.year, end_month = end_date.month, end_day = end_date.day))\n              webContent = response.read().decode('UTF-8')\n              parsed_html = BeautifulSoup(webContent)\n\n              data_table = parsed_html.find_all('tbody')[3].find_all('tr')\n\n              for k in range(len(data_table)):\n                  new_cell_IDs.append(cell_ID[i])\n                  dates.append(data_table[k].find_all('td')[0].text)\n                  snow_water_eq.append(data_table[k].find_all('td')[2].text)\n\n      else:\n          new_cell_IDs.append(cell_ID[i])\n          dates.append('N/A')\n          snow_water_eq.append('N/A')\n\n\n  #time_series = [new_cell_IDs, dates, snow_water_eq]\n  #print(time_series)\n  df = pd.DataFrame({'cell_IDs': new_cell_IDs, 'date_times': dates, 'swe': snow_water_eq})\n  df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv')\n  print(\"Saved\")\n\nget_snotel_time_series()\n",
  "history_output" : "  File \"/Users/joe/gw-workspace/i6L0QnAP3ReA/data_snotel_real_time.py\", line 125\n    df.to_csv(f\"{github_dir}/data/snotel/SNOTEL_time_series_all_grid_eval.csv')\n                                                                               ^\nSyntaxError: EOL while scanning string literal\n",
  "history_begin_time" : 1657722089959,
  "history_end_time" : 1657722090211,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "dfg92z10qm5",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "['C:\\\\Users\\\\BLi\\\\gw-workspace\\\\dfg92z10qm5', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\python39.zip', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\DLLs', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages\\\\Pythonwin']\nhttps://www.nohrsc.noaa.gov/nearest/index.html?city=40.05352381745094%2C-106.04027196859343&county=&l=5&u=e&y=2022&m=5&d=4\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1346, in do_open\n    h.request(req.get_method(), req.selector, req.data, headers,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1285, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1331, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1280, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1040, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 980, in send\n    self.connect()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1454, in connect\n    self.sock = self._context.wrap_socket(self.sock,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\dfg92z10qm5\\data_snotel_real_time.py\", line 22, in <module>\n    response = urllib.request.urlopen(test_noaa_query_url)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 214, in urlopen\n    return opener.open(url, data, timeout)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 517, in open\n    response = self._open(req, data)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 534, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 494, in _call_chain\n    result = func(*args)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1389, in https_open\n    return self.do_open(http.client.HTTPSConnection, req,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1349, in do_open\n    raise URLError(err)\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
  "history_begin_time" : 1657574463921,
  "history_end_time" : 1657574465593,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "fly4oqbxa0u",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "['C:\\\\Users\\\\BLi\\\\gw-workspace\\\\fly4oqbxa0u', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\python39.zip', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\DLLs', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages']\nhttps://www.nohrsc.noaa.gov/nearest/index.html?city=40.05352381745094%2C-106.04027196859343&county=&l=5&u=e&y=2022&m=5&d=4\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1346, in do_open\n    h.request(req.get_method(), req.selector, req.data, headers,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1285, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1331, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1280, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1040, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 980, in send\n    self.connect()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1454, in connect\n    self.sock = self._context.wrap_socket(self.sock,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\fly4oqbxa0u\\data_snotel_real_time.py\", line 22, in <module>\n    response = urllib.request.urlopen(test_noaa_query_url)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 214, in urlopen\n    return opener.open(url, data, timeout)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 517, in open\n    response = self._open(req, data)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 534, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 494, in _call_chain\n    result = func(*args)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1389, in https_open\n    return self.do_open(http.client.HTTPSConnection, req,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1349, in do_open\n    raise URLError(err)\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
  "history_begin_time" : 1657137850931,
  "history_end_time" : 1657137852930,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "6qvra3gzl3z",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "['C:\\\\Users\\\\BLi\\\\gw-workspace\\\\6qvra3gzl3z', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\python39.zip', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\DLLs', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages']\nhttps://www.nohrsc.noaa.gov/nearest/index.html?city=40.05352381745094%2C-106.04027196859343&county=&l=5&u=e&y=2022&m=5&d=4\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1346, in do_open\n    h.request(req.get_method(), req.selector, req.data, headers,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1285, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1331, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1280, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1040, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 980, in send\n    self.connect()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1454, in connect\n    self.sock = self._context.wrap_socket(self.sock,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\6qvra3gzl3z\\data_snotel_real_time.py\", line 22, in <module>\n    response = urllib.request.urlopen(test_noaa_query_url)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 214, in urlopen\n    return opener.open(url, data, timeout)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 517, in open\n    response = self._open(req, data)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 534, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 494, in _call_chain\n    result = func(*args)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1389, in https_open\n    return self.do_open(http.client.HTTPSConnection, req,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1349, in do_open\n    raise URLError(err)\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
  "history_begin_time" : 1656529476236,
  "history_end_time" : 1656529478059,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "hf7i96n8qdq",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "['C:\\\\Users\\\\BLi\\\\gw-workspace\\\\hf7i96n8qdq', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\python39.zip', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\DLLs', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages']\nhttps://www.nohrsc.noaa.gov/nearest/index.html?city=40.05352381745094%2C-106.04027196859343&county=&l=5&u=e&y=2022&m=5&d=4\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1346, in do_open\n    h.request(req.get_method(), req.selector, req.data, headers,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1285, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1331, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1280, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1040, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 980, in send\n    self.connect()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1454, in connect\n    self.sock = self._context.wrap_socket(self.sock,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\hf7i96n8qdq\\data_snotel_real_time.py\", line 22, in <module>\n    response = urllib.request.urlopen(test_noaa_query_url)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 214, in urlopen\n    return opener.open(url, data, timeout)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 517, in open\n    response = self._open(req, data)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 534, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 494, in _call_chain\n    result = func(*args)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1389, in https_open\n    return self.do_open(http.client.HTTPSConnection, req,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1349, in do_open\n    raise URLError(err)\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
  "history_begin_time" : 1656523443380,
  "history_end_time" : 1656523445249,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "xs8c0ikzgce",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "",
  "history_begin_time" : 1656443789179,
  "history_end_time" : 1656443790774,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "grjrkwruh4t",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "['C:\\\\Users\\\\BLi\\\\gw-workspace\\\\grjrkwruh4t', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\python39.zip', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\DLLs', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages']\nhttps://www.nohrsc.noaa.gov/nearest/index.html?city=40.05352381745094%2C-106.04027196859343&county=&l=5&u=e&y=2022&m=5&d=4\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1346, in do_open\n    h.request(req.get_method(), req.selector, req.data, headers,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1285, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1331, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1280, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1040, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 980, in send\n    self.connect()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1454, in connect\n    self.sock = self._context.wrap_socket(self.sock,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\grjrkwruh4t\\data_snotel_real_time.py\", line 22, in <module>\n    response = urllib.request.urlopen(test_noaa_query_url)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 214, in urlopen\n    return opener.open(url, data, timeout)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 517, in open\n    response = self._open(req, data)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 534, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 494, in _call_chain\n    result = func(*args)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1389, in https_open\n    return self.do_open(http.client.HTTPSConnection, req,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1349, in do_open\n    raise URLError(err)\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
  "history_begin_time" : 1656443355914,
  "history_end_time" : 1656443357457,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "Krig9ceQR4BP",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "['C:\\\\Users\\\\BLi\\\\gw-workspace\\\\Krig9ceQR4BP', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\python39.zip', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\DLLs', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages']\nhttps://www.nohrsc.noaa.gov/nearest/index.html?city=40.05352381745094%2C-106.04027196859343&county=&l=5&u=e&y=2022&m=5&d=4\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1346, in do_open\n    h.request(req.get_method(), req.selector, req.data, headers,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1285, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1331, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1280, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1040, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 980, in send\n    self.connect()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1454, in connect\n    self.sock = self._context.wrap_socket(self.sock,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\Krig9ceQR4BP\\data_snotel_real_time.py\", line 22, in <module>\n    response = urllib.request.urlopen(test_noaa_query_url)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 214, in urlopen\n    return opener.open(url, data, timeout)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 517, in open\n    response = self._open(req, data)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 534, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 494, in _call_chain\n    result = func(*args)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1389, in https_open\n    return self.do_open(http.client.HTTPSConnection, req,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1349, in do_open\n    raise URLError(err)\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
  "history_begin_time" : 1656439690069,
  "history_end_time" : 1656439691505,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "avdf0wrrzxf",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "['C:\\\\Users\\\\BLi\\\\gw-workspace\\\\avdf0wrrzxf', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\python39.zip', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\DLLs', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages']\nhttps://www.nohrsc.noaa.gov/nearest/index.html?city=40.05352381745094%2C-106.04027196859343&county=&l=5&u=e&y=2022&m=5&d=4\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1346, in do_open\n    h.request(req.get_method(), req.selector, req.data, headers,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1285, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1331, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1280, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1040, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 980, in send\n    self.connect()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\http\\client.py\", line 1454, in connect\n    self.sock = self._context.wrap_socket(self.sock,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\avdf0wrrzxf\\data_snotel_real_time.py\", line 22, in <module>\n    response = urllib.request.urlopen(test_noaa_query_url)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 214, in urlopen\n    return opener.open(url, data, timeout)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 517, in open\n    response = self._open(req, data)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 534, in _open\n    result = self._call_chain(self.handle_open, protocol, protocol +\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 494, in _call_chain\n    result = func(*args)\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1389, in https_open\n    return self.do_open(http.client.HTTPSConnection, req,\n  File \"C:\\Users\\BLi\\anaconda3\\envs\\snowcast\\lib\\urllib\\request.py\", line 1349, in do_open\n    raise URLError(err)\nurllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1129)>\n",
  "history_begin_time" : 1656439587454,
  "history_end_time" : 1656439589103,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "jynultjxdh6",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "['C:\\\\Users\\\\BLi\\\\gw-workspace\\\\jynultjxdh6', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\python39.zip', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\DLLs', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages']\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\jynultjxdh6\\data_snotel_real_time.py\", line 12, in <module>\n    from BeautifulSoup import BeautifulSoup\nModuleNotFoundError: No module named 'BeautifulSoup'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\jynultjxdh6\\data_snotel_real_time.py\", line 14, in <module>\n    from bs4 import BeautifulSoup\nModuleNotFoundError: No module named 'bs4'\n",
  "history_begin_time" : 1656439457792,
  "history_end_time" : 1656439458892,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "6ew3egddu7v",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "",
  "history_begin_time" : 1656439255912,
  "history_end_time" : 1656439256642,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "hcpcmwjcfb2",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "['C:\\\\Users\\\\BLi\\\\gw-workspace\\\\hcpcmwjcfb2', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\python39.zip', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\DLLs', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast', 'C:\\\\Users\\\\BLi\\\\anaconda3\\\\envs\\\\snowcast\\\\lib\\\\site-packages']\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\hcpcmwjcfb2\\data_snotel_real_time.py\", line 12, in <module>\n    from BeautifulSoup import BeautifulSoup\nModuleNotFoundError: No module named 'BeautifulSoup'\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\hcpcmwjcfb2\\data_snotel_real_time.py\", line 14, in <module>\n    from bs4 import BeautifulSoup\nModuleNotFoundError: No module named 'bs4'\n",
  "history_begin_time" : 1656439115594,
  "history_end_time" : 1656439116745,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "utwj4mlnd2o",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "",
  "history_begin_time" : 1656438438169,
  "history_end_time" : 1656438438987,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "yka5l27a9z5",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\yka5l27a9z5\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656438191590,
  "history_end_time" : 1656438191726,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "wstlrc1sptt",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\wstlrc1sptt\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656437728748,
  "history_end_time" : 1656437728882,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "sbwrgzckr4x",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\sbwrgzckr4x\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656362529452,
  "history_end_time" : 1656362529569,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "s11r2et2ttx",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\s11r2et2ttx\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656361468857,
  "history_end_time" : 1656361468975,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "j24g3qhmpst",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\j24g3qhmpst\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656361419480,
  "history_end_time" : 1656361419595,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "a9x5v64hp9v",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\a9x5v64hp9v\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656361407874,
  "history_end_time" : 1656361407989,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "dhfmq3gyxlj",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\dhfmq3gyxlj\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656360844705,
  "history_end_time" : 1656360844822,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "usumxrviq2p",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\usumxrviq2p\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656360788339,
  "history_end_time" : 1656360788452,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "w8rms6s7e3m",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\w8rms6s7e3m\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656360761764,
  "history_end_time" : 1656360761880,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "z48fzbynm0g",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\z48fzbynm0g\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656360654905,
  "history_end_time" : 1656360655025,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "xltmtlhcotx",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "",
  "history_begin_time" : 1656360395224,
  "history_end_time" : 1656360395319,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0kntxza0b2z",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\0kntxza0b2z\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656360355734,
  "history_end_time" : 1656360355842,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5upf87nnjxp",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\5upf87nnjxp\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656360285152,
  "history_end_time" : 1656360285303,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "nnz1ilqt2hl",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\nnz1ilqt2hl\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656360134344,
  "history_end_time" : 1656360134468,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "jxmqumgxks7",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\jxmqumgxks7\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656349573523,
  "history_end_time" : 1656349573626,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "tln69rgzhbx",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\tln69rgzhbx\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656349419285,
  "history_end_time" : 1656349419358,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "9athluzqoxh",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\9athluzqoxh\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656348499651,
  "history_end_time" : 1656348499764,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "pgz8lmrpeub",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1655310354397,
  "history_end_time" : 1655310354520,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "lr1ja58rjmw",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nsnotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\ndf = snotel_point.get_daily_data(\n    datetime(2020, 1, 2), datetime(2020, 1, 20),\n    [snotel_point.ALLOWED_VARIABLES.SWE]\n)\nprint(df)",
  "history_output" : "                                                                        geometry  ...  datasource\ndatetime                  site                                                    ...            \n2020-01-02 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-03 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-04 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-05 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-06 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-07 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-08 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-09 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-10 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-11 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-12 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-13 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-14 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-15 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-16 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-17 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-18 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-19 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-20 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n[19 rows x 4 columns]\n",
  "history_begin_time" : 1654519487343,
  "history_end_time" : 1654519489546,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "TyafqfQQDnVh",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nsnotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\ndf = snotel_point.get_daily_data(\n    datetime(2020, 1, 2), datetime(2020, 1, 20),\n    [snotel_point.ALLOWED_VARIABLES.SWE]\n)\nprint(df)",
  "history_output" : "                                                                        geometry  ...  datasource\ndatetime                  site                                                    ...            \n2020-01-02 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-03 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-04 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-05 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-06 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-07 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-08 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-09 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-10 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-11 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-12 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-13 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-14 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-15 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-16 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-17 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-18 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-19 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-20 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n[19 rows x 4 columns]\n",
  "history_begin_time" : 1654519412744,
  "history_end_time" : 1654519415600,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7YvDLFJgCKL5",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nsnotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\ndf = snotel_point.get_daily_data(\n    datetime(2020, 1, 2), datetime(2020, 1, 20),\n    [snotel_point.ALLOWED_VARIABLES.SWE]\n)\nprint(df)",
  "history_output" : "                                                                        geometry  ...  datasource\ndatetime                  site                                                    ...            \n2020-01-02 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-03 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-04 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-05 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-06 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-07 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-08 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-09 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-10 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-11 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-12 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-13 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-14 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-15 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-16 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-17 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-18 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-19 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-20 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n[19 rows x 4 columns]\n",
  "history_begin_time" : 1654352628137,
  "history_end_time" : 1654352630624,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rk6VnLnSmQsu",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nsnotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\ndf = snotel_point.get_daily_data(\n    datetime(2020, 1, 2), datetime(2020, 1, 20),\n    [snotel_point.ALLOWED_VARIABLES.SWE]\n)\nprint(df)",
  "history_output" : "                                                                        geometry  ...  datasource\ndatetime                  site                                                    ...            \n2020-01-02 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-03 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-04 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-05 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-06 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-07 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-08 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-09 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-10 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-11 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-12 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-13 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-14 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-15 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-16 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-17 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-18 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-19 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n2020-01-20 08:00:00+00:00 713:CO:SNTL  POINT Z (-107.71389 37.89168 11080.00000)  ...        NRCS\n[19 rows x 4 columns]\n",
  "history_begin_time" : 1654290337491,
  "history_end_time" : 1654290340959,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "lIdSPYNQQx0T",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\nsnotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\ndf = snotel_point.get_daily_data(\n    datetime(2020, 1, 2), datetime(2020, 1, 20),\n    [snotel_point.ALLOWED_VARIABLES.SWE]\n)\nprint(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/lIdSPYNQQx0T/data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1654290321059,
  "history_end_time" : 1654290321135,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "PGQqshu1imog",
  "history_input" : "import os\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\n\nimport ulmo # this library has big trouble in Python 3.9\n\nsites = ulmo.cuahsi.wof.get_sites(wsdlurl)\n\n#Preview first item in dictionary\nprint(next(iter(sites.items())))\n\nsites_df = pd.DataFrame.from_dict(sites, orient='index').dropna()\nprint(sites_df.head())\n\nsites_df['geometry'] = [Point(float(loc['longitude']), float(loc['latitude'])) for loc in sites_df['location']]\n\nsites_df = sites_df.drop(columns='location')\nsites_df = sites_df.astype({\"elevation_m\":float})\n\nprint(sites_df.head())\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL'])\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL']['site_property'])\n\nsites_gdf_all = gpd.GeoDataFrame(sites_df, crs='EPSG:4326')\nprint(sites_gdf_all.head())\n\nprint(sites_gdf_all.shape)\n\n#geojson of state polygons\nstates_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\nstates_gdf = gpd.read_file(states_url)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_all.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\n#This prevents matplotlib from updating the axes extent (states polygons cover larger area than SNOTEL points)\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_gdf_conus = sites_gdf_all[~(sites_gdf_all.index.str.contains('AK'))]\n\nprint(sites_gdf_conus.shape)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_conus.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_fn = 'snotel_conus_sites.json'\nif not os.path.exists(sites_fn):\n    sites_gdf_conus.to_file(sites_fn, driver='GeoJSON')\n\n    ",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/PGQqshu1imog/data_snotel_real_time.py\", line 10, in <module>\n    import ulmo # this library has big trouble in Python 3.9\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/__init__.py\", line 9, in <module>\n    from . import cpc\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/__init__.py\", line 1, in <module>\n    from . import drought\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/drought/__init__.py\", line 8, in <module>\n    from .core import get_data\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/drought/core.py\", line 19, in <module>\n    from ulmo import util\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/util/__init__.py\", line 1, in <module>\n    from .misc import (\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/util/misc.py\", line 8, in <module>\n    import urlparse\nModuleNotFoundError: No module named 'urlparse'\n",
  "history_begin_time" : 1654289821663,
  "history_end_time" : 1654289824479,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "zZDxYhYoYJa0",
  "history_input" : "import os\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\nimport ulmo\n\nsites = ulmo.cuahsi.wof.get_sites(wsdlurl)\n\n#Preview first item in dictionary\nprint(next(iter(sites.items())))\n\nsites_df = pd.DataFrame.from_dict(sites, orient='index').dropna()\nprint(sites_df.head())\n\nsites_df['geometry'] = [Point(float(loc['longitude']), float(loc['latitude'])) for loc in sites_df['location']]\n\nsites_df = sites_df.drop(columns='location')\nsites_df = sites_df.astype({\"elevation_m\":float})\n\nprint(sites_df.head())\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL'])\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL']['site_property'])\n\nsites_gdf_all = gpd.GeoDataFrame(sites_df, crs='EPSG:4326')\nprint(sites_gdf_all.head())\n\nprint(sites_gdf_all.shape)\n\n#geojson of state polygons\nstates_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\nstates_gdf = gpd.read_file(states_url)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_all.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\n#This prevents matplotlib from updating the axes extent (states polygons cover larger area than SNOTEL points)\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_gdf_conus = sites_gdf_all[~(sites_gdf_all.index.str.contains('AK'))]\n\nprint(sites_gdf_conus.shape)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_conus.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_fn = 'snotel_conus_sites.json'\nif not os.path.exists(sites_fn):\n    sites_gdf_conus.to_file(sites_fn, driver='GeoJSON')",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/zZDxYhYoYJa0/data_snotel_real_time.py\", line 9, in <module>\n    import ulmo\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/__init__.py\", line 9, in <module>\n    from . import cpc\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/__init__.py\", line 1, in <module>\n    from . import drought\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/drought/__init__.py\", line 8, in <module>\n    from .core import get_data\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/cpc/drought/core.py\", line 19, in <module>\n    from ulmo import util\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/util/__init__.py\", line 1, in <module>\n    from .misc import (\n  File \"/Users/l21-n02609-comm/opt/anaconda3/lib/python3.9/site-packages/ulmo/util/misc.py\", line 8, in <module>\n    import urlparse\nModuleNotFoundError: No module named 'urlparse'\n",
  "history_begin_time" : 1653916164155,
  "history_end_time" : 1653916166101,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "z8rfe8YRjyRe",
  "history_input" : "import os\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\nimport ulmo\n\nsites = ulmo.cuahsi.wof.get_sites(wsdlurl)\n\n#Preview first item in dictionary\nprint(next(iter(sites.items())))\n\nsites_df = pd.DataFrame.from_dict(sites, orient='index').dropna()\nprint(sites_df.head())\n\nsites_df['geometry'] = [Point(float(loc['longitude']), float(loc['latitude'])) for loc in sites_df['location']]\n\nsites_df = sites_df.drop(columns='location')\nsites_df = sites_df.astype({\"elevation_m\":float})\n\nprint(sites_df.head())\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL'])\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL']['site_property'])\n\nsites_gdf_all = gpd.GeoDataFrame(sites_df, crs='EPSG:4326')\nprint(sites_gdf_all.head())\n\nprint(sites_gdf_all.shape)\n\n#geojson of state polygons\nstates_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\nstates_gdf = gpd.read_file(states_url)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_all.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\n#This prevents matplotlib from updating the axes extent (states polygons cover larger area than SNOTEL points)\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_gdf_conus = sites_gdf_all[~(sites_gdf_all.index.str.contains('AK'))]\n\nprint(sites_gdf_conus.shape)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_conus.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_fn = 'snotel_conus_sites.json'\nif not os.path.exists(sites_fn):\n    sites_gdf_conus.to_file(sites_fn, driver='GeoJSON')",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/z8rfe8YRjyRe/data_snotel_real_time.py\", line 9, in <module>\n    import ulmo\nModuleNotFoundError: No module named 'ulmo'\n",
  "history_begin_time" : 1653916136107,
  "history_end_time" : 1653916139031,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "NzcbQhlCCWbM",
  "history_input" : "import os\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport contextily as ctx\nimport ulmo\n\nsites = ulmo.cuahsi.wof.get_sites(wsdlurl)\n\n#Preview first item in dictionary\nprint(next(iter(sites.items())))\n\nsites_df = pd.DataFrame.from_dict(sites, orient='index').dropna()\nprint(sites_df.head())\n\nsites_df['geometry'] = [Point(float(loc['longitude']), float(loc['latitude'])) for loc in sites_df['location']]\n\nsites_df = sites_df.drop(columns='location')\nsites_df = sites_df.astype({\"elevation_m\":float})\n\nprint(sites_df.head())\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL'])\n\nprint(sites_df.loc['SNOTEL:301_CA_SNTL']['site_property'])\n\nsites_gdf_all = gpd.GeoDataFrame(sites_df, crs='EPSG:4326')\nprint(sites_gdf_all.head())\n\nprint(sites_gdf_all.shape)\n\n#geojson of state polygons\nstates_url = 'http://eric.clst.org/assets/wiki/uploads/Stuff/gz_2010_us_040_00_5m.json'\nstates_gdf = gpd.read_file(states_url)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_all.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\n#This prevents matplotlib from updating the axes extent (states polygons cover larger area than SNOTEL points)\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_gdf_conus = sites_gdf_all[~(sites_gdf_all.index.str.contains('AK'))]\n\nprint(sites_gdf_conus.shape)\n\nf, ax = plt.subplots(figsize=(10,6))\nsites_gdf_conus.plot(ax=ax, column='elevation_m', markersize=3, cmap='inferno', legend=True, legend_kwds={'label': \"Elevation (m)\"})\nax.autoscale(False)\nstates_gdf.plot(ax=ax, facecolor='none', edgecolor='k', alpha=0.3);\n\nsites_fn = 'snotel_conus_sites.json'\nif not os.path.exists(sites_fn):\n    sites_gdf_conus.to_file(sites_fn, driver='GeoJSON')",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/joe/gw-workspace/NzcbQhlCCWbM/data_snotel_real_time.py\", line 8, in <module>\n    import contextily as ctx\nModuleNotFoundError: No module named 'contextily'\n",
  "history_begin_time" : 1653916109409,
  "history_end_time" : 1653916111149,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : null,
  "indicator" : "Done"
},]
