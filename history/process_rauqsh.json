[{
  "history_id" : "mrknszsqabf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667434821978,
  "history_end_time" : 1667434821978,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o0ew209p4en",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667420912874,
  "history_end_time" : 1667420912874,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r0xqrp9hm4h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667420799792,
  "history_end_time" : 1667420799792,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "78pr361q414",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667412122207,
  "history_end_time" : 1667412122207,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xeuxjthngs1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667411534148,
  "history_end_time" : 1667411534148,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u83pzh68brg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667411146168,
  "history_end_time" : 1667411146168,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3727jowscut",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410736680,
  "history_end_time" : 1667410736680,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wtx2g3pf0jc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410696429,
  "history_end_time" : 1667410696429,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6sbyean0hkr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410660488,
  "history_end_time" : 1667410690573,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7kf2y64sevl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410652147,
  "history_end_time" : 1667410703744,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ae203v9thn5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410575661,
  "history_end_time" : 1667410651119,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s1aunw03z2d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410544195,
  "history_end_time" : 1667410623997,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "flahfvgs03k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667410384339,
  "history_end_time" : 1667410556063,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "iumf16ipepn",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1667409178319,
  "history_end_time" : 1667409178840,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "0awcme1ndgh",
  "history_input" : "# 2020.06.09-Changed for building GhostNet\n#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n\"\"\"\nCreates a GhostNet Model as defined in:\nhttps://arxiv.org/abs/1911.11907\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport math\nimport pandas as pd\nimport json\nimport geojson\nimport geopandas as gpd\nimport os.path\nfrom datetime import datetime\n\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\n__all__ = ['ghostnet']\n\nall_ready_file = f\"{github_dir}/data/ready_for_training/all_ready.csv\"\n\n\ndef preprocessing(self):\n    all_ready_pd = pd.read_csv(self.all_ready_file, header=0, index_col=0)\n    all_ready_pd = all_ready_pd.fillna(10000)  # replace all nan with 10000\n\n'''custom torch dataset here'''\n\ntrain, test = train_test_split(all_ready_pd, test_size=0.2)\ntrain_x, train_y = train[\n                                 ['year', 'm', 'doy', 'ndsi', 'grd', 'eto', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd',\n                                  'vs', 'lat', 'lon', 'elevation', 'aspect', 'curvature', 'slope', 'eastness',\n                                  'northness']].to_numpy().astype('float'), train['swe'].to_numpy().astype('float')\ntest_x, test_y = test[['year', 'm', 'doy', 'ndsi', 'grd', 'eto', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd',\n                                 'vs', 'lat', 'lon', 'elevation', 'aspect', 'curvature', 'slope', 'eastness',\n                                 'northness']].to_numpy().astype('float'), test['swe'].to_numpy().astype('float')\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef hard_sigmoid(x, inplace: bool = False):\n    if inplace:\n        return x.add_(3.).clamp_(0., 6.).div_(6.)\n    else:\n        return F.relu6(x + 3.) / 6.\n\n\nclass SqueezeExcite(nn.Module):\n    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None,\n                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):\n        super(SqueezeExcite, self).__init__()\n        self.gate_fn = gate_fn\n        reduced_chs = _make_divisible((reduced_base_chs or in_chs) * se_ratio, divisor)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_reduce = nn.Conv2d(in_chs, reduced_chs, 1, bias=True)\n        self.act1 = act_layer(inplace=True)\n        self.conv_expand = nn.Conv2d(reduced_chs, in_chs, 1, bias=True)\n\n    def forward(self, x):\n        x_se = self.avg_pool(x)\n        x_se = self.conv_reduce(x_se)\n        x_se = self.act1(x_se)\n        x_se = self.conv_expand(x_se)\n        x = x * self.gate_fn(x_se)\n        return x\n\n\nclass ConvBnAct(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size,\n                 stride=1, act_layer=nn.ReLU):\n        super(ConvBnAct, self).__init__()\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size // 2, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_chs)\n        self.act1 = act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        return x\n\n\nclass GhostModule(nn.Module):\n    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):\n        super(GhostModule, self).__init__()\n        self.oup = oup\n        init_channels = math.ceil(oup / ratio)\n        new_channels = init_channels * (ratio - 1)\n\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size // 2, bias=False),\n            nn.BatchNorm2d(init_channels),\n            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n        )\n\n        self.cheap_operation = nn.Sequential(\n            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size // 2, groups=init_channels, bias=False),\n            nn.BatchNorm2d(new_channels),\n            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n        )\n\n    def forward(self, x):\n        x1 = self.primary_conv(x)\n        x2 = self.cheap_operation(x1)\n        out = torch.cat([x1, x2], dim=1)\n        return out[:, :self.oup, :, :]\n\n\nclass GhostBottleneck(nn.Module):\n    \"\"\" Ghost bottleneck w/ optional SE\"\"\"\n\n    def __init__(self, in_chs, mid_chs, out_chs, dw_kernel_size=3,\n                 stride=1, act_layer=nn.ReLU, se_ratio=0.):\n        super(GhostBottleneck, self).__init__()\n        has_se = se_ratio is not None and se_ratio > 0.\n        self.stride = stride\n\n        # Point-wise expansion\n        self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)\n\n        # Depth-wise convolution\n        if self.stride > 1:\n            self.conv_dw = nn.Conv2d(mid_chs, mid_chs, dw_kernel_size, stride=stride,\n                                     padding=(dw_kernel_size - 1) // 2,\n                                     groups=mid_chs, bias=False)\n            self.bn_dw = nn.BatchNorm2d(mid_chs)\n\n        # Squeeze-and-excitation\n        if has_se:\n            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio)\n        else:\n            self.se = None\n\n        # Point-wise linear projection\n        self.ghost2 = GhostModule(mid_chs, out_chs, relu=False)\n\n        # shortcut\n        if (in_chs == out_chs and self.stride == 1):\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_chs, in_chs, dw_kernel_size, stride=stride,\n                          padding=(dw_kernel_size - 1) // 2, groups=in_chs, bias=False),\n                nn.BatchNorm2d(in_chs),\n                nn.Conv2d(in_chs, out_chs, 1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_chs),\n            )\n\n    def forward(self, x):\n        residual = x\n\n        # 1st ghost bottleneck\n        x = self.ghost1(x)\n\n        # Depth-wise convolution\n        if self.stride > 1:\n            x = self.conv_dw(x)\n            x = self.bn_dw(x)\n\n        # Squeeze-and-excitation\n        if self.se is not None:\n            x = self.se(x)\n\n        # 2nd ghost bottleneck\n        x = self.ghost2(x)\n\n        x += self.shortcut(residual)\n        return x\n\n\nclass GhostNet(nn.Module):\n    def __init__(self, cfgs, num_classes=1000, width=1.0, dropout=0.2):\n        super(GhostNet, self).__init__()\n        # setting of inverted residual blocks\n        self.cfgs = cfgs\n        self.dropout = dropout\n\n        # building first layer\n        output_channel = _make_divisible(16 * width, 4)\n        self.conv_stem = nn.Conv2d(21, output_channel, 3, 2, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(output_channel)\n        self.act1 = nn.ReLU(inplace=True)\n        input_channel = output_channel\n\n        # building inverted residual blocks\n        stages = []\n        block = GhostBottleneck\n        for cfg in self.cfgs:\n            layers = []\n            for k, exp_size, c, se_ratio, s in cfg:\n                output_channel = _make_divisible(c * width, 4)\n                hidden_channel = _make_divisible(exp_size * width, 4)\n                layers.append(block(input_channel, hidden_channel, output_channel, k, s,\n                                    se_ratio=se_ratio))\n                input_channel = output_channel\n            stages.append(nn.Sequential(*layers))\n\n        output_channel = _make_divisible(exp_size * width, 4)\n        stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))\n        input_channel = output_channel\n\n        self.blocks = nn.Sequential(*stages)\n\n        # building last several layers\n        output_channel = 1280\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.conv_head = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=True)\n        self.act2 = nn.ReLU(inplace=True)\n        self.classifier = nn.Linear(output_channel, num_classes)\n\n    def forward(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.blocks(x)\n        x = self.global_pool(x)\n        x = self.conv_head(x)\n        x = self.act2(x)\n        x = x.view(x.size(0), -1)\n        if self.dropout > 0.:\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\ndef ghostnet(**kwargs):\n    \"\"\"\n    Constructs a GhostNet model\n    \"\"\"\n    cfgs = [\n        # k, t, c, SE, s\n        # stage1\n        [[3, 16, 16, 0, 1]],\n        # stage2\n        [[3, 48, 24, 0, 2]],\n        [[3, 72, 24, 0, 1]],\n        # stage3\n        [[5, 72, 40, 0.25, 2]],\n        [[5, 120, 40, 0.25, 1]],\n        # stage4\n        [[3, 240, 80, 0, 2]],\n        [[3, 200, 80, 0, 1],\n         [3, 184, 80, 0, 1],\n         [3, 184, 80, 0, 1],\n         [3, 480, 112, 0.25, 1],\n         [3, 672, 112, 0.25, 1]\n         ],\n        # stage5\n        [[5, 672, 160, 0.25, 2]],\n        [[5, 960, 160, 0, 1],\n         [5, 960, 160, 0.25, 1],\n         [5, 960, 160, 0, 1],\n         [5, 960, 160, 0.25, 1]\n         ]\n    ]\n    return GhostNet(cfgs, **kwargs)\n\n\nif __name__ == '__main__':\n    model = ghostnet()\n    model.eval()\n    print(model)\n    input = torch.randn(32, 3, 320, 256)\n    y = model(input)\n    print(y.size())",
  "history_output" : "",
  "history_begin_time" : 1657574447357,
  "history_end_time" : 1657574448091,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "abkHAAamue93",
  "history_input" : "# 2020.06.09-Changed for building GhostNet\n#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n\"\"\"\nCreates a GhostNet Model as defined in:\nhttps://arxiv.org/abs/1911.11907\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport math\nimport json\nimport geojson\nimport geopandas as gpd\nimport os.path\nfrom datetime import datetime\n\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\n\n__all__ = ['ghost_net']\n\nall_ready_file = f\"{github_dir}/data/ready_for_training/all_ready.csv\"\n\ndef preprocessing(self):\n\tall_ready_pd = pd.read_csv(self.all_ready_file, header=0, index_col=0)\n \tall_ready_pd = all_ready_pd.fillna(10000) # replace all nan with 10000\n    train, test = train_test_split(all_ready_pd, test_size=0.2)\n    self.train_x, self.train_y = train[['year','m','doy','ndsi','grd','eto','pr','rmax','rmin','tmmn','tmmx','vpd','vs','lat','lon','elevation','aspect','curvature','slope','eastness','northness']].to_numpy().astype('float'), train['swe'].to_numpy().astype('float')\n    self.test_x, self.test_y = test[['year','m','doy','ndsi','grd','eto','pr','rmax','rmin','tmmn','tmmx','vpd','vs','lat','lon','elevation','aspect','curvature','slope','eastness','northness']].to_numpy().astype('float'), test['swe'].to_numpy().astype('float')\n\ndef _make_divisible(v, divisor, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef hard_sigmoid(x, inplace: bool = False):\n    if inplace:\n        return x.add_(3.).clamp_(0., 6.).div_(6.)\n    else:\n        return F.relu6(x + 3.) / 6.\n\n\nclass SqueezeExcite(nn.Module):\n    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None,\n                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):\n        super(SqueezeExcite, self).__init__()\n        self.gate_fn = gate_fn\n        reduced_chs = _make_divisible((reduced_base_chs or in_chs) * se_ratio, divisor)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_reduce = nn.Conv2d(in_chs, reduced_chs, 1, bias=True)\n        self.act1 = act_layer(inplace=True)\n        self.conv_expand = nn.Conv2d(reduced_chs, in_chs, 1, bias=True)\n\n    def forward(self, x):\n        x_se = self.avg_pool(x)\n        x_se = self.conv_reduce(x_se)\n        x_se = self.act1(x_se)\n        x_se = self.conv_expand(x_se)\n        x = x * self.gate_fn(x_se)\n        return x    \n\n    \nclass ConvBnAct(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size,\n                 stride=1, act_layer=nn.ReLU):\n        super(ConvBnAct, self).__init__()\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size//2, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_chs)\n        self.act1 = act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        return x\n\n\nclass GhostModule(nn.Module):\n    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):\n        super(GhostModule, self).__init__()\n        self.oup = oup\n        init_channels = math.ceil(oup / ratio)\n        new_channels = init_channels*(ratio-1)\n\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),\n            nn.BatchNorm2d(init_channels),\n            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n        )\n\n        self.cheap_operation = nn.Sequential(\n            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),\n            nn.BatchNorm2d(new_channels),\n            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n        )\n\n    def forward(self, x):\n        x1 = self.primary_conv(x)\n        x2 = self.cheap_operation(x1)\n        out = torch.cat([x1,x2], dim=1)\n        return out[:,:self.oup,:,:]\n\n\nclass GhostBottleneck(nn.Module):\n    \"\"\" Ghost bottleneck w/ optional SE\"\"\"\n\n    def __init__(self, in_chs, mid_chs, out_chs, dw_kernel_size=3,\n                 stride=1, act_layer=nn.ReLU, se_ratio=0.):\n        super(GhostBottleneck, self).__init__()\n        has_se = se_ratio is not None and se_ratio > 0.\n        self.stride = stride\n\n        # Point-wise expansion\n        self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)\n\n        # Depth-wise convolution\n        if self.stride > 1:\n            self.conv_dw = nn.Conv2d(mid_chs, mid_chs, dw_kernel_size, stride=stride,\n                             padding=(dw_kernel_size-1)//2,\n                             groups=mid_chs, bias=False)\n            self.bn_dw = nn.BatchNorm2d(mid_chs)\n\n        # Squeeze-and-excitation\n        if has_se:\n            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio)\n        else:\n            self.se = None\n\n        # Point-wise linear projection\n        self.ghost2 = GhostModule(mid_chs, out_chs, relu=False)\n        \n        # shortcut\n        if (in_chs == out_chs and self.stride == 1):\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_chs, in_chs, dw_kernel_size, stride=stride,\n                       padding=(dw_kernel_size-1)//2, groups=in_chs, bias=False),\n                nn.BatchNorm2d(in_chs),\n                nn.Conv2d(in_chs, out_chs, 1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_chs),\n            )\n\n\n    def forward(self, x):\n        residual = x\n\n        # 1st ghost bottleneck\n        x = self.ghost1(x)\n\n        # Depth-wise convolution\n        if self.stride > 1:\n            x = self.conv_dw(x)\n            x = self.bn_dw(x)\n\n        # Squeeze-and-excitation\n        if self.se is not None:\n            x = self.se(x)\n\n        # 2nd ghost bottleneck\n        x = self.ghost2(x)\n        \n        x += self.shortcut(residual)\n        return x\n\n\nclass GhostNet(nn.Module):\n    def __init__(self, cfgs, num_classes=1000, width=1.0, dropout=0.2):\n        super(GhostNet, self).__init__()\n        # setting of inverted residual blocks\n        self.cfgs = cfgs\n        self.dropout = dropout\n\n        # building first layer\n        output_channel = _make_divisible(16 * width, 4)\n        self.conv_stem = nn.Conv2d(21, output_channel, 3, 2, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(output_channel)\n        self.act1 = nn.ReLU(inplace=True)\n        input_channel = output_channel\n\n        # building inverted residual blocks\n        stages = []\n        block = GhostBottleneck\n        for cfg in self.cfgs:\n            layers = []\n            for k, exp_size, c, se_ratio, s in cfg:\n                output_channel = _make_divisible(c * width, 4)\n                hidden_channel = _make_divisible(exp_size * width, 4)\n                layers.append(block(input_channel, hidden_channel, output_channel, k, s,\n                              se_ratio=se_ratio))\n                input_channel = output_channel\n            stages.append(nn.Sequential(*layers))\n\n        output_channel = _make_divisible(exp_size * width, 4)\n        stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))\n        input_channel = output_channel\n        \n        self.blocks = nn.Sequential(*stages)        \n\n        # building last several layers\n        output_channel = 1280\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.conv_head = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=True)\n        self.act2 = nn.ReLU(inplace=True)\n        self.classifier = nn.Linear(output_channel, num_classes)\n\n    def forward(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.blocks(x)\n        x = self.global_pool(x)\n        x = self.conv_head(x)\n        x = self.act2(x)\n        x = x.view(x.size(0), -1)\n        if self.dropout > 0.:\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\ndef ghostnet(**kwargs):\n    \"\"\"\n    Constructs a GhostNet model\n    \"\"\"\n    cfgs = [\n        # k, t, c, SE, s \n        # stage1\n        [[3,  16,  16, 0, 1]],\n        # stage2\n        [[3,  48,  24, 0, 2]],\n        [[3,  72,  24, 0, 1]],\n        # stage3\n        [[5,  72,  40, 0.25, 2]],\n        [[5, 120,  40, 0.25, 1]],\n        # stage4\n        [[3, 240,  80, 0, 2]],\n        [[3, 200,  80, 0, 1],\n         [3, 184,  80, 0, 1],\n         [3, 184,  80, 0, 1],\n         [3, 480, 112, 0.25, 1],\n         [3, 672, 112, 0.25, 1]\n        ],\n        # stage5\n        [[5, 672, 160, 0.25, 2]],\n        [[5, 960, 160, 0, 1],\n         [5, 960, 160, 0.25, 1],\n         [5, 960, 160, 0, 1],\n         [5, 960, 160, 0.25, 1]\n        ]\n    ]\n    return GhostNet(cfgs, **kwargs)\n\n\nif __name__=='__main__':\n    model = ghostnet()\n    model.eval()\n    print(model)\n    input = torch.randn(32,3,320,256)\n    y = model(input)\n    print(y.size())",
  "history_output" : "  File \"C:\\Users\\BLi\\gw-workspace\\abkHAAamue93\\model_creation_ghostnet.py\", line 28\n    all_ready_pd = all_ready_pd.fillna(10000) # replace all nan with 10000\nTabError: inconsistent use of tabs and spaces in indentation\n",
  "history_begin_time" : 1657541112526,
  "history_end_time" : 1657541112633,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "4j2wdn5genb",
  "history_input" : "# 2020.06.09-Changed for building GhostNet\n#            Huawei Technologies Co., Ltd. <foss@huawei.com>\n\"\"\"\nCreates a GhostNet Model as defined in:\nhttps://arxiv.org/abs/1911.11907\nModified from https://github.com/huawei-noah/Efficient-AI-Backbones\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport math\nimport json\nimport geojson\nimport geopandas as gpd\nimport os.path\nfrom datetime import datetime\n\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\n\n__all__ = ['ghost_net']\n\nall_ready_file = f\"{github_dir}/data/ready_for_training/all_ready.csv\"\n\ndef preprocessing(self):\n\tall_ready_pd = pd.read_csv(self.all_ready_file, header=0, index_col=0)\n \tall_ready_pd = all_ready_pd.fillna(10000) # replace all nan with 10000\n    train, test = train_test_split(all_ready_pd, test_size=0.2)\n    self.train_x, self.train_y = train[['year','m','doy','ndsi','grd','eto','pr','rmax','rmin','tmmn','tmmx','vpd','vs','lat','lon','elevation','aspect','curvature','slope','eastness','northness']].to_numpy().astype('float'), train['swe'].to_numpy().astype('float')\n    self.test_x, self.test_y = test[['year','m','doy','ndsi','grd','eto','pr','rmax','rmin','tmmn','tmmx','vpd','vs','lat','lon','elevation','aspect','curvature','slope','eastness','northness']].to_numpy().astype('float'), test['swe'].to_numpy().astype('float')\n\ndef _make_divisible(v, divisor, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef hard_sigmoid(x, inplace: bool = False):\n    if inplace:\n        return x.add_(3.).clamp_(0., 6.).div_(6.)\n    else:\n        return F.relu6(x + 3.) / 6.\n\n\nclass SqueezeExcite(nn.Module):\n    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None,\n                 act_layer=nn.ReLU, gate_fn=hard_sigmoid, divisor=4, **_):\n        super(SqueezeExcite, self).__init__()\n        self.gate_fn = gate_fn\n        reduced_chs = _make_divisible((reduced_base_chs or in_chs) * se_ratio, divisor)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_reduce = nn.Conv2d(in_chs, reduced_chs, 1, bias=True)\n        self.act1 = act_layer(inplace=True)\n        self.conv_expand = nn.Conv2d(reduced_chs, in_chs, 1, bias=True)\n\n    def forward(self, x):\n        x_se = self.avg_pool(x)\n        x_se = self.conv_reduce(x_se)\n        x_se = self.act1(x_se)\n        x_se = self.conv_expand(x_se)\n        x = x * self.gate_fn(x_se)\n        return x    \n\n    \nclass ConvBnAct(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size,\n                 stride=1, act_layer=nn.ReLU):\n        super(ConvBnAct, self).__init__()\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, kernel_size//2, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_chs)\n        self.act1 = act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        return x\n\n\nclass GhostModule(nn.Module):\n    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1, relu=True):\n        super(GhostModule, self).__init__()\n        self.oup = oup\n        init_channels = math.ceil(oup / ratio)\n        new_channels = init_channels*(ratio-1)\n\n        self.primary_conv = nn.Sequential(\n            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),\n            nn.BatchNorm2d(init_channels),\n            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n        )\n\n        self.cheap_operation = nn.Sequential(\n            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),\n            nn.BatchNorm2d(new_channels),\n            nn.ReLU(inplace=True) if relu else nn.Sequential(),\n        )\n\n    def forward(self, x):\n        x1 = self.primary_conv(x)\n        x2 = self.cheap_operation(x1)\n        out = torch.cat([x1,x2], dim=1)\n        return out[:,:self.oup,:,:]\n\n\nclass GhostBottleneck(nn.Module):\n    \"\"\" Ghost bottleneck w/ optional SE\"\"\"\n\n    def __init__(self, in_chs, mid_chs, out_chs, dw_kernel_size=3,\n                 stride=1, act_layer=nn.ReLU, se_ratio=0.):\n        super(GhostBottleneck, self).__init__()\n        has_se = se_ratio is not None and se_ratio > 0.\n        self.stride = stride\n\n        # Point-wise expansion\n        self.ghost1 = GhostModule(in_chs, mid_chs, relu=True)\n\n        # Depth-wise convolution\n        if self.stride > 1:\n            self.conv_dw = nn.Conv2d(mid_chs, mid_chs, dw_kernel_size, stride=stride,\n                             padding=(dw_kernel_size-1)//2,\n                             groups=mid_chs, bias=False)\n            self.bn_dw = nn.BatchNorm2d(mid_chs)\n\n        # Squeeze-and-excitation\n        if has_se:\n            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio)\n        else:\n            self.se = None\n\n        # Point-wise linear projection\n        self.ghost2 = GhostModule(mid_chs, out_chs, relu=False)\n        \n        # shortcut\n        if (in_chs == out_chs and self.stride == 1):\n            self.shortcut = nn.Sequential()\n        else:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_chs, in_chs, dw_kernel_size, stride=stride,\n                       padding=(dw_kernel_size-1)//2, groups=in_chs, bias=False),\n                nn.BatchNorm2d(in_chs),\n                nn.Conv2d(in_chs, out_chs, 1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_chs),\n            )\n\n\n    def forward(self, x):\n        residual = x\n\n        # 1st ghost bottleneck\n        x = self.ghost1(x)\n\n        # Depth-wise convolution\n        if self.stride > 1:\n            x = self.conv_dw(x)\n            x = self.bn_dw(x)\n\n        # Squeeze-and-excitation\n        if self.se is not None:\n            x = self.se(x)\n\n        # 2nd ghost bottleneck\n        x = self.ghost2(x)\n        \n        x += self.shortcut(residual)\n        return x\n\n\nclass GhostNet(nn.Module):\n    def __init__(self, cfgs, num_classes=1000, width=1.0, dropout=0.2):\n        super(GhostNet, self).__init__()\n        # setting of inverted residual blocks\n        self.cfgs = cfgs\n        self.dropout = dropout\n\n        # building first layer\n        output_channel = _make_divisible(16 * width, 4)\n        self.conv_stem = nn.Conv2d(3, output_channel, 3, 2, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(output_channel)\n        self.act1 = nn.ReLU(inplace=True)\n        input_channel = output_channel\n\n        # building inverted residual blocks\n        stages = []\n        block = GhostBottleneck\n        for cfg in self.cfgs:\n            layers = []\n            for k, exp_size, c, se_ratio, s in cfg:\n                output_channel = _make_divisible(c * width, 4)\n                hidden_channel = _make_divisible(exp_size * width, 4)\n                layers.append(block(input_channel, hidden_channel, output_channel, k, s,\n                              se_ratio=se_ratio))\n                input_channel = output_channel\n            stages.append(nn.Sequential(*layers))\n\n        output_channel = _make_divisible(exp_size * width, 4)\n        stages.append(nn.Sequential(ConvBnAct(input_channel, output_channel, 1)))\n        input_channel = output_channel\n        \n        self.blocks = nn.Sequential(*stages)        \n\n        # building last several layers\n        output_channel = 1280\n        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.conv_head = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=True)\n        self.act2 = nn.ReLU(inplace=True)\n        self.classifier = nn.Linear(output_channel, num_classes)\n\n    def forward(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.blocks(x)\n        x = self.global_pool(x)\n        x = self.conv_head(x)\n        x = self.act2(x)\n        x = x.view(x.size(0), -1)\n        if self.dropout > 0.:\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\ndef ghostnet(**kwargs):\n    \"\"\"\n    Constructs a GhostNet model\n    \"\"\"\n    cfgs = [\n        # k, t, c, SE, s \n        # stage1\n        [[3,  16,  16, 0, 1]],\n        # stage2\n        [[3,  48,  24, 0, 2]],\n        [[3,  72,  24, 0, 1]],\n        # stage3\n        [[5,  72,  40, 0.25, 2]],\n        [[5, 120,  40, 0.25, 1]],\n        # stage4\n        [[3, 240,  80, 0, 2]],\n        [[3, 200,  80, 0, 1],\n         [3, 184,  80, 0, 1],\n         [3, 184,  80, 0, 1],\n         [3, 480, 112, 0.25, 1],\n         [3, 672, 112, 0.25, 1]\n        ],\n        # stage5\n        [[5, 672, 160, 0.25, 2]],\n        [[5, 960, 160, 0, 1],\n         [5, 960, 160, 0.25, 1],\n         [5, 960, 160, 0, 1],\n         [5, 960, 160, 0.25, 1]\n        ]\n    ]\n    return GhostNet(cfgs, **kwargs)\n\n\nif __name__=='__main__':\n    model = ghostnet()\n    model.eval()\n    print(model)\n    input = torch.randn(32,3,320,256)\n    y = model(input)\n    print(y.size())",
  "history_output" : "  File \"C:\\Users\\BLi\\gw-workspace\\4j2wdn5genb\\model_creation_ghostnet.py\", line 29\n    all_ready_pd = all_ready_pd.fillna(10000) # replace all nan with 10000\nTabError: inconsistent use of tabs and spaces in indentation\n",
  "history_begin_time" : 1657137842134,
  "history_end_time" : 1657137842838,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "79j7f98rflh",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656529468681,
  "history_end_time" : 1656529469378,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6ib5cllrqcn",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656523431892,
  "history_end_time" : 1656523432638,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "jyzyzqy4dci",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656443560749,
  "history_end_time" : 1656443560878,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "59z6xadpavx",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656443330966,
  "history_end_time" : 1656443331077,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0rujbvvffnf",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656439581218,
  "history_end_time" : 1656439581322,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "o558v8spndm",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656439445370,
  "history_end_time" : 1656439445475,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "r3ncrl5b59l",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656439250389,
  "history_end_time" : 1656439250468,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "v28mv1m96yz",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656439108042,
  "history_end_time" : 1656439108215,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ivqhlus33hz",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656438432001,
  "history_end_time" : 1656438432642,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "4o29ppf5d6k",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656438188219,
  "history_end_time" : 1656438188355,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6e6oxpwwh5b",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656437721014,
  "history_end_time" : 1656437721233,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "yavr7evy4hh",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656362524326,
  "history_end_time" : 1656362524537,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "755s8g7hir5",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "",
  "history_begin_time" : 1656361463042,
  "history_end_time" : 1656361463111,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "amu610vchje",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656361409699,
  "history_end_time" : 1656361409803,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "poiq7n8frh1",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656361404409,
  "history_end_time" : 1656361404525,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "tfjdoqb6h9u",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656361165147,
  "history_end_time" : 1656361165971,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8g08ee671o0",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n\nThe remote endpoint was in state [TEXT_FULL_WRITING] which is an invalid state for called method",
  "history_begin_time" : 1656360838994,
  "history_end_time" : 1656360839696,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "dnou5gxfzjg",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656360780989,
  "history_end_time" : 1656360781072,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "3z0i03g970p",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656360758052,
  "history_end_time" : 1656360758216,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "fj4f2uezx0h",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656360650397,
  "history_end_time" : 1656360651201,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "u0k18n8yz8y",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656360392568,
  "history_end_time" : 1656360392696,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "haxae6j29nv",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656360351367,
  "history_end_time" : 1656360351478,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "wpkyb9kwjd5",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656360282812,
  "history_end_time" : 1656360282935,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "p0bbv7gt7nw",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656360130758,
  "history_end_time" : 1656360130868,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "quu6d9kzs22",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656349569384,
  "history_end_time" : 1656349569466,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "rg6v83qj33d",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656349416854,
  "history_end_time" : 1656349416939,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "dml2rztch3j",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656348494820,
  "history_end_time" : 1656348495495,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "zxfwuj3y59s",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1655310349924,
  "history_end_time" : 1655310350149,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "h042zy6hyji",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1654519479519,
  "history_end_time" : 1654519479609,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "mz84jbtgwm3",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1647826055194,
  "history_end_time" : 1647826056790,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "o0l0qqtsx3d",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1647225904341,
  "history_end_time" : 1647225904868,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8q26s1jl494",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1646692933327,
  "history_end_time" : 1646692933454,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "1rmvv195irq",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1646604591652,
  "history_end_time" : 1646604591849,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8ss5xpxs2kr",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1646269655907,
  "history_end_time" : 1646269656331,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "a8zwg3dfabt",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "tar: /home/zsun/idjcvRcyvRGH4R217cUR64p5wb.tar: Cannot open: No such file or directory\ntar: Error is not recoverable: exiting now\nCreate GhostNet\n",
  "history_begin_time" : 1645977639146,
  "history_end_time" : 1645977640441,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "je29f6",
  "indicator" : "Done"
},{
  "history_id" : "5azrtox1ieh",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "rm: cannot remove '/home/zsun/idjcvRcyvRGH4R217cUR64p5wb.tar': No such file or directory\nCreate GhostNet\n",
  "history_begin_time" : 1645977432490,
  "history_end_time" : 1645977433837,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "je29f6",
  "indicator" : "Done"
},{
  "history_id" : "jns36hwdkcp",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1642977854464,
  "history_end_time" : 1642977854618,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "226355jc7n3",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1642969772666,
  "history_end_time" : 1642969772921,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "9wgqf0992k2",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1642455226722,
  "history_end_time" : 1642455226823,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "p6rpx0olh5y",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1642454674465,
  "history_end_time" : 1642454674599,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},]
